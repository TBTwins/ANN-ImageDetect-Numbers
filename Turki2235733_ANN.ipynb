{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa4a95b",
   "metadata": {},
   "source": [
    "#### Name: Turki Abdulaziz Alghamdi\n",
    "#### ID: 2235733"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3601b",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4920fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fd458",
   "metadata": {},
   "source": [
    "# Set My Own Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b1dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible results\n",
    "np.random.seed(105)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd0213",
   "metadata": {},
   "source": [
    "# Creating ANN Class with Needed Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e548809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, input_size):\n",
    "        # First hidden layer\n",
    "        self.w1 = 0.009 * np.random.randn(input_size, 32)\n",
    "        self.b1 = np.zeros((1, 32))\n",
    "        # Second hidden layer\n",
    "        self.w2 = 0.009 * np.random.randn(32, 16)\n",
    "        self.b2 = np.zeros((1, 16))\n",
    "        # Output layer\n",
    "        self.w3 = 0.009 * np.random.randn(16, 10)\n",
    "        self.b3 = np.zeros((1, 10))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # First hidden layer\n",
    "        z1 = np.dot(X, self.w1) + self.b1\n",
    "        a1 = np.maximum(0, z1)  # ReLU\n",
    "        # Second hidden layer\n",
    "        z2 = np.dot(a1, self.w2) + self.b2\n",
    "        a2 = np.maximum(0, z2)  # ReLU\n",
    "        # Output layer\n",
    "        z3 = np.dot(a2, self.w3) + self.b3\n",
    "        exp_z = np.exp(z3 - np.max(z3, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)  # Softmax\n",
    "    \n",
    "    def loss(self, pred, y):\n",
    "        return -np.mean(np.log(np.clip(pred[range(len(pred)), y], 1e-7, 1)))\n",
    "    \n",
    "    def save(self, name):\n",
    "        np.savez(name, w1=self.w1, b1=self.b1, w2=self.w2, b2=self.b2, w3=self.w3, b3=self.b3)\n",
    "    \n",
    "    def load(self, name):\n",
    "        data = np.load(name)\n",
    "        self.w1, self.b1 = data['w1'], data['b1']\n",
    "        self.w2, self.b2 = data['w2'], data['b2']\n",
    "        self.w3, self.b3 = data['w3'], data['b3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5fadd",
   "metadata": {},
   "source": [
    "# Creating Data Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c830824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    X, y = [], []\n",
    "    for digit in range(10):\n",
    "        folder = os.path.join(path, str(digit))\n",
    "        if os.path.exists(folder):\n",
    "            for file in os.listdir(folder):\n",
    "                if file.endswith(('.png')):\n",
    "                    img = Image.open(os.path.join(folder, file))\n",
    "                    arr = np.array(img, dtype=np.float64)\n",
    "                    flat_arr = arr.flatten()\n",
    "                    if flat_arr.max() > 1: flat_arr /= 255.0\n",
    "                    X.append(flat_arr)\n",
    "                    y.append(digit)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abaae2",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be552424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1980 images\n",
      "Image shape: (784,)\n",
      "Classes: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_data(\"Ai2_Dataset/Turki G\")\n",
    "print(f\"Loaded {len(X)} images\")\n",
    "print(f\"Image shape: {X[0].shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6351b",
   "metadata": {},
   "source": [
    "# **Creating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6b54d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with input size: 784\n",
      "Architecture: 784 -> 32 -> 16 -> 10\n"
     ]
    }
   ],
   "source": [
    "#Create model\n",
    "model = ANN(X.shape[1])\n",
    "print(f\"Model created with input size: {X.shape[1]}\")\n",
    "print(f\"Architecture: {X.shape[1]} -> 32 -> 16 -> 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df796d",
   "metadata": {},
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55486c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Step 0: Loss=2.303, Acc=0.100\n",
      "Step 10: Loss=2.302, Acc=0.100\n",
      "Step 11: Loss=2.302, Acc=0.100\n",
      "Step 15: Loss=2.302, Acc=0.108\n",
      "Step 18: Loss=2.302, Acc=0.100\n",
      "Step 20: Loss=2.302, Acc=0.100\n",
      "Step 26: Loss=2.302, Acc=0.130\n",
      "Step 27: Loss=2.302, Acc=0.128\n",
      "Step 31: Loss=2.302, Acc=0.099\n",
      "Step 33: Loss=2.302, Acc=0.107\n",
      "Step 34: Loss=2.302, Acc=0.100\n",
      "Step 35: Loss=2.302, Acc=0.108\n",
      "Step 43: Loss=2.302, Acc=0.087\n",
      "Step 44: Loss=2.301, Acc=0.122\n",
      "Step 47: Loss=2.301, Acc=0.114\n",
      "Step 48: Loss=2.301, Acc=0.135\n",
      "Step 53: Loss=2.301, Acc=0.101\n",
      "Step 55: Loss=2.301, Acc=0.100\n",
      "Step 56: Loss=2.300, Acc=0.100\n",
      "Step 58: Loss=2.299, Acc=0.100\n",
      "Step 60: Loss=2.299, Acc=0.100\n",
      "Step 61: Loss=2.299, Acc=0.104\n",
      "Step 64: Loss=2.298, Acc=0.114\n",
      "Step 68: Loss=2.298, Acc=0.110\n",
      "Step 69: Loss=2.298, Acc=0.105\n",
      "Step 72: Loss=2.297, Acc=0.102\n",
      "Step 73: Loss=2.297, Acc=0.101\n",
      "Step 74: Loss=2.297, Acc=0.102\n",
      "Step 76: Loss=2.296, Acc=0.100\n",
      "Step 80: Loss=2.296, Acc=0.101\n",
      "Step 81: Loss=2.295, Acc=0.101\n",
      "Step 82: Loss=2.295, Acc=0.115\n",
      "Step 83: Loss=2.295, Acc=0.110\n",
      "Step 85: Loss=2.295, Acc=0.106\n",
      "Step 98: Loss=2.294, Acc=0.105\n",
      "Step 102: Loss=2.294, Acc=0.128\n",
      "Step 109: Loss=2.293, Acc=0.152\n",
      "Step 110: Loss=2.293, Acc=0.159\n",
      "Step 112: Loss=2.292, Acc=0.161\n",
      "Step 118: Loss=2.292, Acc=0.144\n",
      "Step 125: Loss=2.291, Acc=0.140\n",
      "Step 127: Loss=2.290, Acc=0.134\n",
      "Step 129: Loss=2.289, Acc=0.146\n",
      "Step 131: Loss=2.289, Acc=0.146\n",
      "Step 132: Loss=2.288, Acc=0.150\n",
      "Step 145: Loss=2.288, Acc=0.116\n",
      "Step 148: Loss=2.287, Acc=0.123\n",
      "Step 150: Loss=2.287, Acc=0.112\n",
      "Step 153: Loss=2.287, Acc=0.107\n",
      "Step 157: Loss=2.286, Acc=0.123\n",
      "Step 158: Loss=2.284, Acc=0.103\n",
      "Step 160: Loss=2.283, Acc=0.110\n",
      "Step 165: Loss=2.282, Acc=0.110\n",
      "Step 172: Loss=2.281, Acc=0.115\n",
      "Step 179: Loss=2.280, Acc=0.118\n",
      "Step 185: Loss=2.276, Acc=0.122\n",
      "Step 187: Loss=2.276, Acc=0.133\n",
      "Step 189: Loss=2.276, Acc=0.110\n",
      "Step 191: Loss=2.273, Acc=0.111\n",
      "Step 194: Loss=2.273, Acc=0.140\n",
      "Step 196: Loss=2.272, Acc=0.197\n",
      "Step 197: Loss=2.270, Acc=0.144\n",
      "Step 206: Loss=2.269, Acc=0.155\n",
      "Step 207: Loss=2.267, Acc=0.125\n",
      "Step 210: Loss=2.266, Acc=0.127\n",
      "Step 214: Loss=2.265, Acc=0.133\n",
      "Step 215: Loss=2.265, Acc=0.136\n",
      "Step 221: Loss=2.262, Acc=0.130\n",
      "Step 224: Loss=2.262, Acc=0.130\n",
      "Step 229: Loss=2.259, Acc=0.125\n",
      "Step 231: Loss=2.258, Acc=0.144\n",
      "Step 237: Loss=2.257, Acc=0.120\n",
      "Step 238: Loss=2.254, Acc=0.141\n",
      "Step 240: Loss=2.252, Acc=0.156\n",
      "Step 250: Loss=2.251, Acc=0.174\n",
      "Step 255: Loss=2.249, Acc=0.170\n",
      "Step 261: Loss=2.247, Acc=0.176\n",
      "Step 264: Loss=2.247, Acc=0.172\n",
      "Step 267: Loss=2.246, Acc=0.163\n",
      "Step 268: Loss=2.243, Acc=0.208\n",
      "Step 270: Loss=2.243, Acc=0.185\n",
      "Step 271: Loss=2.241, Acc=0.183\n",
      "Step 273: Loss=2.240, Acc=0.217\n",
      "Step 289: Loss=2.240, Acc=0.211\n",
      "Step 307: Loss=2.239, Acc=0.198\n",
      "Step 308: Loss=2.235, Acc=0.185\n",
      "Step 314: Loss=2.235, Acc=0.198\n",
      "Step 321: Loss=2.233, Acc=0.151\n",
      "Step 323: Loss=2.232, Acc=0.163\n",
      "Step 332: Loss=2.229, Acc=0.187\n",
      "Step 340: Loss=2.228, Acc=0.201\n",
      "Step 342: Loss=2.227, Acc=0.182\n",
      "Step 349: Loss=2.218, Acc=0.227\n",
      "Step 354: Loss=2.218, Acc=0.199\n",
      "Step 356: Loss=2.214, Acc=0.227\n",
      "Step 357: Loss=2.213, Acc=0.221\n",
      "Step 374: Loss=2.210, Acc=0.211\n",
      "Step 386: Loss=2.206, Acc=0.202\n",
      "Step 396: Loss=2.204, Acc=0.196\n",
      "Step 404: Loss=2.199, Acc=0.219\n",
      "Step 412: Loss=2.197, Acc=0.221\n",
      "Step 419: Loss=2.197, Acc=0.220\n",
      "Step 420: Loss=2.191, Acc=0.210\n",
      "Step 431: Loss=2.190, Acc=0.227\n",
      "Step 433: Loss=2.189, Acc=0.211\n",
      "Step 434: Loss=2.189, Acc=0.213\n",
      "Step 437: Loss=2.189, Acc=0.214\n",
      "Step 443: Loss=2.189, Acc=0.193\n",
      "Step 448: Loss=2.188, Acc=0.232\n",
      "Step 450: Loss=2.188, Acc=0.248\n",
      "Step 454: Loss=2.186, Acc=0.268\n",
      "Step 456: Loss=2.185, Acc=0.246\n",
      "Step 457: Loss=2.178, Acc=0.250\n",
      "Step 458: Loss=2.176, Acc=0.268\n",
      "Step 461: Loss=2.173, Acc=0.325\n",
      "Step 466: Loss=2.172, Acc=0.334\n",
      "Step 471: Loss=2.166, Acc=0.343\n",
      "Step 473: Loss=2.164, Acc=0.339\n",
      "Step 483: Loss=2.163, Acc=0.320\n",
      "Step 485: Loss=2.154, Acc=0.336\n",
      "Step 505: Loss=2.148, Acc=0.335\n",
      "Step 510: Loss=2.147, Acc=0.319\n",
      "Step 517: Loss=2.143, Acc=0.316\n",
      "Step 532: Loss=2.140, Acc=0.307\n",
      "Step 539: Loss=2.136, Acc=0.290\n",
      "Step 548: Loss=2.133, Acc=0.315\n",
      "Step 550: Loss=2.131, Acc=0.332\n",
      "Step 562: Loss=2.130, Acc=0.375\n",
      "Step 564: Loss=2.129, Acc=0.361\n",
      "Step 570: Loss=2.126, Acc=0.340\n",
      "Step 573: Loss=2.122, Acc=0.306\n",
      "Step 579: Loss=2.121, Acc=0.312\n",
      "Step 582: Loss=2.114, Acc=0.304\n",
      "Step 590: Loss=2.114, Acc=0.243\n",
      "Step 591: Loss=2.109, Acc=0.266\n",
      "Step 592: Loss=2.107, Acc=0.275\n",
      "Step 599: Loss=2.103, Acc=0.291\n",
      "Step 602: Loss=2.102, Acc=0.290\n",
      "Step 604: Loss=2.097, Acc=0.270\n",
      "Step 613: Loss=2.091, Acc=0.285\n",
      "Step 619: Loss=2.091, Acc=0.298\n",
      "Step 628: Loss=2.085, Acc=0.254\n",
      "Step 630: Loss=2.083, Acc=0.311\n",
      "Step 641: Loss=2.077, Acc=0.247\n",
      "Step 645: Loss=2.069, Acc=0.293\n",
      "Step 654: Loss=2.063, Acc=0.372\n",
      "Step 655: Loss=2.058, Acc=0.357\n",
      "Step 657: Loss=2.053, Acc=0.342\n",
      "Step 665: Loss=2.052, Acc=0.353\n",
      "Step 668: Loss=2.052, Acc=0.363\n",
      "Step 672: Loss=2.047, Acc=0.360\n",
      "Step 673: Loss=2.045, Acc=0.358\n",
      "Step 680: Loss=2.044, Acc=0.277\n",
      "Step 682: Loss=2.042, Acc=0.330\n",
      "Step 685: Loss=2.038, Acc=0.347\n",
      "Step 688: Loss=2.037, Acc=0.376\n",
      "Step 692: Loss=2.037, Acc=0.391\n",
      "Step 693: Loss=2.036, Acc=0.379\n",
      "Step 697: Loss=2.031, Acc=0.409\n",
      "Step 703: Loss=2.025, Acc=0.417\n",
      "Step 706: Loss=2.022, Acc=0.383\n",
      "Step 718: Loss=2.022, Acc=0.399\n",
      "Step 730: Loss=2.015, Acc=0.359\n",
      "Step 731: Loss=2.014, Acc=0.357\n",
      "Step 738: Loss=2.013, Acc=0.378\n",
      "Step 751: Loss=2.008, Acc=0.398\n",
      "Step 753: Loss=1.998, Acc=0.434\n",
      "Step 762: Loss=1.994, Acc=0.432\n",
      "Step 769: Loss=1.988, Acc=0.419\n",
      "Step 776: Loss=1.987, Acc=0.429\n",
      "Step 777: Loss=1.984, Acc=0.423\n",
      "Step 783: Loss=1.980, Acc=0.430\n",
      "Step 791: Loss=1.975, Acc=0.413\n",
      "Step 803: Loss=1.974, Acc=0.417\n",
      "Step 807: Loss=1.967, Acc=0.454\n",
      "Step 810: Loss=1.963, Acc=0.425\n",
      "Step 815: Loss=1.958, Acc=0.434\n",
      "Step 816: Loss=1.951, Acc=0.420\n",
      "Step 831: Loss=1.950, Acc=0.413\n",
      "Step 832: Loss=1.949, Acc=0.406\n",
      "Step 837: Loss=1.946, Acc=0.466\n",
      "Step 849: Loss=1.944, Acc=0.459\n",
      "Step 866: Loss=1.944, Acc=0.440\n",
      "Step 883: Loss=1.943, Acc=0.465\n",
      "Step 889: Loss=1.943, Acc=0.402\n",
      "Step 896: Loss=1.941, Acc=0.395\n",
      "Step 907: Loss=1.938, Acc=0.404\n",
      "Step 908: Loss=1.936, Acc=0.440\n",
      "Step 909: Loss=1.935, Acc=0.452\n",
      "Step 912: Loss=1.928, Acc=0.405\n",
      "Step 915: Loss=1.920, Acc=0.431\n",
      "Step 920: Loss=1.917, Acc=0.387\n",
      "Step 935: Loss=1.915, Acc=0.391\n",
      "Step 937: Loss=1.914, Acc=0.378\n",
      "Step 950: Loss=1.911, Acc=0.358\n",
      "Step 952: Loss=1.909, Acc=0.360\n",
      "Step 953: Loss=1.903, Acc=0.391\n",
      "Step 954: Loss=1.892, Acc=0.353\n",
      "Step 957: Loss=1.891, Acc=0.336\n",
      "Step 959: Loss=1.890, Acc=0.319\n",
      "Step 960: Loss=1.872, Acc=0.330\n",
      "Step 967: Loss=1.871, Acc=0.366\n",
      "Step 973: Loss=1.865, Acc=0.394\n",
      "Step 975: Loss=1.865, Acc=0.359\n",
      "Step 979: Loss=1.864, Acc=0.401\n",
      "Step 990: Loss=1.859, Acc=0.417\n",
      "Step 992: Loss=1.854, Acc=0.445\n",
      "Step 993: Loss=1.853, Acc=0.415\n",
      "Step 1002: Loss=1.853, Acc=0.399\n",
      "Step 1010: Loss=1.852, Acc=0.384\n",
      "Step 1011: Loss=1.836, Acc=0.403\n",
      "Step 1016: Loss=1.834, Acc=0.419\n",
      "Step 1021: Loss=1.834, Acc=0.409\n",
      "Step 1024: Loss=1.832, Acc=0.409\n",
      "Step 1038: Loss=1.831, Acc=0.394\n",
      "Step 1040: Loss=1.828, Acc=0.391\n",
      "Step 1041: Loss=1.827, Acc=0.390\n",
      "Step 1045: Loss=1.820, Acc=0.404\n",
      "Step 1058: Loss=1.818, Acc=0.380\n",
      "Step 1059: Loss=1.804, Acc=0.399\n",
      "Step 1060: Loss=1.799, Acc=0.403\n",
      "Step 1072: Loss=1.799, Acc=0.412\n",
      "Step 1079: Loss=1.795, Acc=0.430\n",
      "Step 1097: Loss=1.793, Acc=0.418\n",
      "Step 1100: Loss=1.788, Acc=0.437\n",
      "Step 1102: Loss=1.787, Acc=0.435\n",
      "Step 1108: Loss=1.784, Acc=0.425\n",
      "Step 1113: Loss=1.773, Acc=0.439\n",
      "Step 1115: Loss=1.769, Acc=0.457\n",
      "Step 1126: Loss=1.768, Acc=0.452\n",
      "Step 1138: Loss=1.765, Acc=0.428\n",
      "Step 1146: Loss=1.761, Acc=0.395\n",
      "Step 1162: Loss=1.754, Acc=0.414\n",
      "Step 1174: Loss=1.753, Acc=0.418\n",
      "Step 1185: Loss=1.750, Acc=0.422\n",
      "Step 1193: Loss=1.748, Acc=0.450\n",
      "Step 1196: Loss=1.747, Acc=0.455\n",
      "Step 1197: Loss=1.744, Acc=0.476\n",
      "Step 1198: Loss=1.736, Acc=0.449\n",
      "Step 1242: Loss=1.736, Acc=0.496\n",
      "Step 1248: Loss=1.733, Acc=0.473\n",
      "Step 1252: Loss=1.725, Acc=0.473\n",
      "Step 1260: Loss=1.716, Acc=0.473\n",
      "Step 1261: Loss=1.706, Acc=0.457\n",
      "Step 1293: Loss=1.706, Acc=0.456\n",
      "Step 1301: Loss=1.694, Acc=0.457\n",
      "Step 1306: Loss=1.689, Acc=0.475\n",
      "Step 1322: Loss=1.685, Acc=0.486\n",
      "Step 1326: Loss=1.684, Acc=0.474\n",
      "Step 1332: Loss=1.676, Acc=0.499\n",
      "Step 1336: Loss=1.670, Acc=0.487\n",
      "Step 1338: Loss=1.665, Acc=0.505\n",
      "Step 1364: Loss=1.663, Acc=0.480\n",
      "Step 1379: Loss=1.661, Acc=0.501\n",
      "Step 1394: Loss=1.660, Acc=0.493\n",
      "Step 1396: Loss=1.656, Acc=0.453\n",
      "Step 1406: Loss=1.653, Acc=0.459\n",
      "Step 1409: Loss=1.652, Acc=0.480\n",
      "Step 1426: Loss=1.649, Acc=0.478\n",
      "Step 1438: Loss=1.647, Acc=0.488\n",
      "Step 1446: Loss=1.643, Acc=0.469\n",
      "Step 1454: Loss=1.636, Acc=0.476\n",
      "Step 1460: Loss=1.631, Acc=0.469\n",
      "Step 1461: Loss=1.627, Acc=0.498\n",
      "Step 1469: Loss=1.626, Acc=0.482\n",
      "Step 1477: Loss=1.625, Acc=0.504\n",
      "Step 1496: Loss=1.624, Acc=0.498\n",
      "Step 1503: Loss=1.616, Acc=0.496\n",
      "Step 1505: Loss=1.615, Acc=0.501\n",
      "Step 1539: Loss=1.614, Acc=0.518\n",
      "Step 1552: Loss=1.612, Acc=0.494\n",
      "Step 1557: Loss=1.601, Acc=0.500\n",
      "Step 1583: Loss=1.597, Acc=0.501\n",
      "Step 1584: Loss=1.591, Acc=0.503\n",
      "Step 1586: Loss=1.590, Acc=0.494\n",
      "Step 1589: Loss=1.583, Acc=0.495\n",
      "Step 1595: Loss=1.579, Acc=0.509\n",
      "Step 1624: Loss=1.578, Acc=0.498\n",
      "Step 1631: Loss=1.577, Acc=0.508\n",
      "Step 1636: Loss=1.576, Acc=0.494\n",
      "Step 1638: Loss=1.575, Acc=0.501\n",
      "Step 1640: Loss=1.566, Acc=0.511\n",
      "Step 1650: Loss=1.561, Acc=0.516\n",
      "Step 1664: Loss=1.553, Acc=0.507\n",
      "Step 1676: Loss=1.550, Acc=0.504\n",
      "Step 1680: Loss=1.545, Acc=0.520\n",
      "Step 1694: Loss=1.545, Acc=0.531\n",
      "Step 1695: Loss=1.543, Acc=0.540\n",
      "Step 1700: Loss=1.536, Acc=0.545\n",
      "Step 1746: Loss=1.533, Acc=0.539\n",
      "Step 1750: Loss=1.523, Acc=0.534\n",
      "Step 1761: Loss=1.511, Acc=0.561\n",
      "Step 1778: Loss=1.511, Acc=0.558\n",
      "Step 1793: Loss=1.510, Acc=0.540\n",
      "Step 1797: Loss=1.510, Acc=0.538\n",
      "Step 1835: Loss=1.508, Acc=0.550\n",
      "Step 1855: Loss=1.488, Acc=0.555\n",
      "Step 1881: Loss=1.486, Acc=0.564\n",
      "Step 1929: Loss=1.484, Acc=0.550\n",
      "Step 1935: Loss=1.479, Acc=0.551\n",
      "Step 1981: Loss=1.478, Acc=0.553\n",
      "Step 1984: Loss=1.475, Acc=0.548\n",
      "Step 1985: Loss=1.474, Acc=0.546\n",
      "Step 1988: Loss=1.472, Acc=0.553\n",
      "Step 2010: Loss=1.468, Acc=0.532\n",
      "Step 2020: Loss=1.464, Acc=0.531\n",
      "Step 2021: Loss=1.458, Acc=0.547\n",
      "Step 2083: Loss=1.457, Acc=0.533\n",
      "Step 2089: Loss=1.446, Acc=0.535\n",
      "Step 2120: Loss=1.442, Acc=0.527\n",
      "Step 2126: Loss=1.439, Acc=0.503\n",
      "Step 2134: Loss=1.435, Acc=0.514\n",
      "Step 2182: Loss=1.433, Acc=0.545\n",
      "Step 2190: Loss=1.423, Acc=0.570\n",
      "Step 2191: Loss=1.422, Acc=0.571\n",
      "Step 2197: Loss=1.421, Acc=0.554\n",
      "Step 2209: Loss=1.409, Acc=0.552\n",
      "Step 2212: Loss=1.405, Acc=0.568\n",
      "Step 2222: Loss=1.388, Acc=0.566\n",
      "Step 2224: Loss=1.383, Acc=0.574\n",
      "Step 2262: Loss=1.382, Acc=0.553\n",
      "Step 2263: Loss=1.382, Acc=0.538\n",
      "Step 2267: Loss=1.380, Acc=0.561\n",
      "Step 2282: Loss=1.379, Acc=0.573\n",
      "Step 2304: Loss=1.374, Acc=0.555\n",
      "Step 2305: Loss=1.369, Acc=0.558\n",
      "Step 2320: Loss=1.362, Acc=0.556\n",
      "Step 2351: Loss=1.361, Acc=0.558\n",
      "Step 2357: Loss=1.360, Acc=0.567\n",
      "Step 2387: Loss=1.360, Acc=0.578\n",
      "Step 2394: Loss=1.338, Acc=0.568\n",
      "Step 2397: Loss=1.337, Acc=0.574\n",
      "Step 2398: Loss=1.329, Acc=0.604\n",
      "Step 2409: Loss=1.316, Acc=0.601\n",
      "Step 2480: Loss=1.314, Acc=0.597\n",
      "Step 2496: Loss=1.313, Acc=0.575\n",
      "Step 2512: Loss=1.301, Acc=0.594\n",
      "Step 2525: Loss=1.300, Acc=0.587\n",
      "Step 2529: Loss=1.293, Acc=0.609\n",
      "Step 2601: Loss=1.288, Acc=0.599\n",
      "Step 2612: Loss=1.283, Acc=0.596\n",
      "Step 2629: Loss=1.281, Acc=0.602\n",
      "Step 2652: Loss=1.281, Acc=0.606\n",
      "Step 2667: Loss=1.265, Acc=0.609\n",
      "Step 2773: Loss=1.263, Acc=0.626\n",
      "Step 2778: Loss=1.259, Acc=0.626\n",
      "Step 2785: Loss=1.257, Acc=0.619\n",
      "Step 2817: Loss=1.256, Acc=0.617\n",
      "Step 2834: Loss=1.253, Acc=0.608\n",
      "Step 2855: Loss=1.242, Acc=0.612\n",
      "Step 2859: Loss=1.241, Acc=0.607\n",
      "Step 2860: Loss=1.236, Acc=0.605\n",
      "Step 2862: Loss=1.233, Acc=0.611\n",
      "Step 2869: Loss=1.227, Acc=0.613\n",
      "Step 2875: Loss=1.224, Acc=0.623\n",
      "Step 2949: Loss=1.219, Acc=0.620\n",
      "Step 2950: Loss=1.212, Acc=0.613\n",
      "Step 2960: Loss=1.206, Acc=0.620\n",
      "Step 2966: Loss=1.203, Acc=0.622\n",
      "Step 2976: Loss=1.194, Acc=0.633\n",
      "Step 2983: Loss=1.193, Acc=0.626\n",
      "Step 2987: Loss=1.189, Acc=0.634\n",
      "Step 2997: Loss=1.188, Acc=0.631\n",
      "Step 2999: Loss=1.185, Acc=0.621\n",
      "Step 3004: Loss=1.183, Acc=0.621\n",
      "Step 3009: Loss=1.180, Acc=0.621\n",
      "Step 3017: Loss=1.176, Acc=0.626\n",
      "Step 3024: Loss=1.175, Acc=0.625\n",
      "Step 3039: Loss=1.158, Acc=0.635\n",
      "Step 3060: Loss=1.158, Acc=0.629\n",
      "Step 3061: Loss=1.157, Acc=0.637\n",
      "Step 3068: Loss=1.154, Acc=0.611\n",
      "Step 3079: Loss=1.154, Acc=0.618\n",
      "Step 3114: Loss=1.150, Acc=0.629\n",
      "Step 3126: Loss=1.144, Acc=0.619\n",
      "Step 3132: Loss=1.132, Acc=0.632\n",
      "Step 3141: Loss=1.127, Acc=0.632\n",
      "Step 3185: Loss=1.123, Acc=0.628\n",
      "Step 3229: Loss=1.121, Acc=0.632\n",
      "Step 3289: Loss=1.116, Acc=0.627\n",
      "Step 3412: Loss=1.114, Acc=0.639\n",
      "Step 3414: Loss=1.113, Acc=0.637\n",
      "Step 3469: Loss=1.109, Acc=0.639\n",
      "Step 3503: Loss=1.108, Acc=0.640\n",
      "Step 3520: Loss=1.106, Acc=0.621\n",
      "Step 3584: Loss=1.103, Acc=0.623\n",
      "Step 3591: Loss=1.100, Acc=0.625\n",
      "Step 3605: Loss=1.097, Acc=0.647\n",
      "Step 3621: Loss=1.085, Acc=0.650\n",
      "Step 3632: Loss=1.076, Acc=0.638\n",
      "Step 3662: Loss=1.074, Acc=0.627\n",
      "Step 3678: Loss=1.073, Acc=0.634\n",
      "Step 3687: Loss=1.070, Acc=0.644\n",
      "Step 3734: Loss=1.067, Acc=0.662\n",
      "Step 3743: Loss=1.064, Acc=0.655\n",
      "Step 3803: Loss=1.064, Acc=0.649\n",
      "Step 3830: Loss=1.061, Acc=0.637\n",
      "Step 3842: Loss=1.061, Acc=0.640\n",
      "Step 3864: Loss=1.060, Acc=0.650\n",
      "Step 3868: Loss=1.052, Acc=0.657\n",
      "Step 3878: Loss=1.046, Acc=0.641\n",
      "Step 3896: Loss=1.045, Acc=0.649\n",
      "Step 3904: Loss=1.043, Acc=0.651\n",
      "Step 3906: Loss=1.036, Acc=0.647\n",
      "Step 4021: Loss=1.025, Acc=0.653\n",
      "Step 4028: Loss=1.024, Acc=0.655\n",
      "Step 4043: Loss=1.022, Acc=0.663\n",
      "Step 4064: Loss=1.016, Acc=0.662\n",
      "Step 4068: Loss=1.012, Acc=0.667\n",
      "Step 4103: Loss=1.005, Acc=0.673\n",
      "Step 4114: Loss=1.004, Acc=0.652\n",
      "Step 4142: Loss=1.000, Acc=0.663\n",
      "Step 4145: Loss=0.987, Acc=0.671\n",
      "Step 4146: Loss=0.986, Acc=0.659\n",
      "Step 4147: Loss=0.980, Acc=0.657\n",
      "Step 4224: Loss=0.980, Acc=0.663\n",
      "Step 4241: Loss=0.978, Acc=0.680\n",
      "Step 4288: Loss=0.971, Acc=0.688\n",
      "Step 4401: Loss=0.970, Acc=0.686\n",
      "Step 4406: Loss=0.969, Acc=0.676\n",
      "Step 4410: Loss=0.965, Acc=0.682\n",
      "Step 4444: Loss=0.963, Acc=0.688\n",
      "Step 4445: Loss=0.962, Acc=0.684\n",
      "Step 4452: Loss=0.958, Acc=0.694\n",
      "Step 4483: Loss=0.957, Acc=0.692\n",
      "Step 4507: Loss=0.956, Acc=0.706\n",
      "Step 4515: Loss=0.956, Acc=0.695\n",
      "Step 4523: Loss=0.955, Acc=0.676\n",
      "Step 4606: Loss=0.954, Acc=0.689\n",
      "Step 4635: Loss=0.948, Acc=0.671\n",
      "Step 4644: Loss=0.945, Acc=0.670\n",
      "Step 4645: Loss=0.944, Acc=0.678\n",
      "Step 4663: Loss=0.943, Acc=0.688\n",
      "Step 4672: Loss=0.943, Acc=0.688\n",
      "Step 4674: Loss=0.939, Acc=0.680\n",
      "Step 4734: Loss=0.937, Acc=0.704\n",
      "Step 4794: Loss=0.933, Acc=0.722\n",
      "Step 4831: Loss=0.931, Acc=0.719\n",
      "Step 4838: Loss=0.928, Acc=0.723\n",
      "Step 4864: Loss=0.923, Acc=0.737\n",
      "Step 4913: Loss=0.922, Acc=0.712\n",
      "Step 4919: Loss=0.920, Acc=0.702\n",
      "Step 4934: Loss=0.909, Acc=0.709\n",
      "Step 4992: Loss=0.906, Acc=0.706\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "print(\"Starting training...\")\n",
    "best_loss = 999\n",
    "for i in range(5000):\n",
    "    # Random weight updates for all layers\n",
    "    model.w1 += 0.009 * np.random.randn(*model.w1.shape)\n",
    "    model.b1 += 0.009 * np.random.randn(*model.b1.shape)\n",
    "    model.w2 += 0.009 * np.random.randn(*model.w2.shape)\n",
    "    model.b2 += 0.009 * np.random.randn(*model.b2.shape)\n",
    "    model.w3 += 0.009 * np.random.randn(*model.w3.shape)\n",
    "    model.b3 += 0.009 * np.random.randn(*model.b3.shape)\n",
    "\n",
    "    pred = model.forward(X)\n",
    "    loss = model.loss(pred, y)\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        model.save(\"Turki2235733Model.npz\")\n",
    "        acc = np.mean(np.argmax(pred, axis=1) == y)\n",
    "        print(f\"Step {i}: Loss={loss:.3f}, Acc={acc:.3f}\")\n",
    "    else:\n",
    "        model.load(\"Turki2235733Model.npz\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac193741",
   "metadata": {},
   "source": [
    "# **Loading Result Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c10f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(\"Turki2235733Model.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccb841",
   "metadata": {},
   "source": [
    "# **Test on shared test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3239585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shared test set...\n",
      "Shared Test Accuracy: 0.740\n"
     ]
    }
   ],
   "source": [
    "test_path = \"Ai2_Dataset/SharedTesting\"\n",
    "if os.path.exists(test_path):\n",
    "    print(\"Loading shared test set...\")\n",
    "    test_X, test_y = load_data(test_path)\n",
    "    test_pred = model.forward(test_X)\n",
    "    test_acc = np.mean(np.argmax(test_pred, axis=1) == test_y)\n",
    "    print(f\"Shared Test Accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfccc61d",
   "metadata": {},
   "source": [
    "# **Print every prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8cba0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Predicted=1, Actual=0\n",
      "Sample 1: Predicted=0, Actual=0\n",
      "Sample 2: Predicted=0, Actual=0\n",
      "Sample 3: Predicted=0, Actual=0\n",
      "Sample 4: Predicted=6, Actual=0\n",
      "Sample 5: Predicted=0, Actual=0\n",
      "Sample 6: Predicted=3, Actual=0\n",
      "Sample 7: Predicted=0, Actual=0\n",
      "Sample 8: Predicted=0, Actual=0\n",
      "Sample 9: Predicted=0, Actual=0\n",
      "Sample 10: Predicted=1, Actual=1\n",
      "Sample 11: Predicted=1, Actual=1\n",
      "Sample 12: Predicted=1, Actual=1\n",
      "Sample 13: Predicted=1, Actual=1\n",
      "Sample 14: Predicted=1, Actual=1\n",
      "Sample 15: Predicted=1, Actual=1\n",
      "Sample 16: Predicted=1, Actual=1\n",
      "Sample 17: Predicted=1, Actual=1\n",
      "Sample 18: Predicted=1, Actual=1\n",
      "Sample 19: Predicted=1, Actual=1\n",
      "Sample 20: Predicted=7, Actual=2\n",
      "Sample 21: Predicted=2, Actual=2\n",
      "Sample 22: Predicted=2, Actual=2\n",
      "Sample 23: Predicted=2, Actual=2\n",
      "Sample 24: Predicted=2, Actual=2\n",
      "Sample 25: Predicted=8, Actual=2\n",
      "Sample 26: Predicted=2, Actual=2\n",
      "Sample 27: Predicted=2, Actual=2\n",
      "Sample 28: Predicted=2, Actual=2\n",
      "Sample 29: Predicted=2, Actual=2\n",
      "Sample 30: Predicted=3, Actual=3\n",
      "Sample 31: Predicted=0, Actual=3\n",
      "Sample 32: Predicted=4, Actual=3\n",
      "Sample 33: Predicted=3, Actual=3\n",
      "Sample 34: Predicted=0, Actual=3\n",
      "Sample 35: Predicted=3, Actual=3\n",
      "Sample 36: Predicted=3, Actual=3\n",
      "Sample 37: Predicted=3, Actual=3\n",
      "Sample 38: Predicted=3, Actual=3\n",
      "Sample 39: Predicted=0, Actual=3\n",
      "Sample 40: Predicted=2, Actual=4\n",
      "Sample 41: Predicted=1, Actual=4\n",
      "Sample 42: Predicted=4, Actual=4\n",
      "Sample 43: Predicted=4, Actual=4\n",
      "Sample 44: Predicted=4, Actual=4\n",
      "Sample 45: Predicted=6, Actual=4\n",
      "Sample 46: Predicted=4, Actual=4\n",
      "Sample 47: Predicted=4, Actual=4\n",
      "Sample 48: Predicted=2, Actual=4\n",
      "Sample 49: Predicted=4, Actual=4\n",
      "Sample 50: Predicted=5, Actual=5\n",
      "Sample 51: Predicted=5, Actual=5\n",
      "Sample 52: Predicted=0, Actual=5\n",
      "Sample 53: Predicted=5, Actual=5\n",
      "Sample 54: Predicted=5, Actual=5\n",
      "Sample 55: Predicted=5, Actual=5\n",
      "Sample 56: Predicted=5, Actual=5\n",
      "Sample 57: Predicted=5, Actual=5\n",
      "Sample 58: Predicted=5, Actual=5\n",
      "Sample 59: Predicted=5, Actual=5\n",
      "Sample 60: Predicted=6, Actual=6\n",
      "Sample 61: Predicted=1, Actual=6\n",
      "Sample 62: Predicted=6, Actual=6\n",
      "Sample 63: Predicted=6, Actual=6\n",
      "Sample 64: Predicted=6, Actual=6\n",
      "Sample 65: Predicted=6, Actual=6\n",
      "Sample 66: Predicted=6, Actual=6\n",
      "Sample 67: Predicted=6, Actual=6\n",
      "Sample 68: Predicted=6, Actual=6\n",
      "Sample 69: Predicted=6, Actual=6\n",
      "Sample 70: Predicted=7, Actual=7\n",
      "Sample 71: Predicted=7, Actual=7\n",
      "Sample 72: Predicted=7, Actual=7\n",
      "Sample 73: Predicted=2, Actual=7\n",
      "Sample 74: Predicted=7, Actual=7\n",
      "Sample 75: Predicted=7, Actual=7\n",
      "Sample 76: Predicted=7, Actual=7\n",
      "Sample 77: Predicted=7, Actual=7\n",
      "Sample 78: Predicted=7, Actual=7\n",
      "Sample 79: Predicted=3, Actual=7\n",
      "Sample 80: Predicted=8, Actual=8\n",
      "Sample 81: Predicted=8, Actual=8\n",
      "Sample 82: Predicted=8, Actual=8\n",
      "Sample 83: Predicted=8, Actual=8\n",
      "Sample 84: Predicted=8, Actual=8\n",
      "Sample 85: Predicted=8, Actual=8\n",
      "Sample 86: Predicted=6, Actual=8\n",
      "Sample 87: Predicted=8, Actual=8\n",
      "Sample 88: Predicted=8, Actual=8\n",
      "Sample 89: Predicted=8, Actual=8\n",
      "Sample 90: Predicted=6, Actual=9\n",
      "Sample 91: Predicted=9, Actual=9\n",
      "Sample 92: Predicted=6, Actual=9\n",
      "Sample 93: Predicted=6, Actual=9\n",
      "Sample 94: Predicted=6, Actual=9\n",
      "Sample 95: Predicted=4, Actual=9\n",
      "Sample 96: Predicted=4, Actual=9\n",
      "Sample 97: Predicted=3, Actual=9\n",
      "Sample 98: Predicted=9, Actual=9\n",
      "Sample 99: Predicted=6, Actual=9\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for i, pred in enumerate(test_pred):\n",
    "    predicted_class = np.argmax(pred)\n",
    "    actual_class = test_y[i]\n",
    "    print(f\"Sample {i}: Predicted={predicted_class}, Actual={actual_class}\")\n",
    "else:\n",
    "    print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
