{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa4a95b",
   "metadata": {},
   "source": [
    "#### Name: Alaa Othman Labban\n",
    "#### ID: 2235858"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3601b",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4920fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fd458",
   "metadata": {},
   "source": [
    "# Set My Own Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b1dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible results\n",
    "np.random.seed(67)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd0213",
   "metadata": {},
   "source": [
    "# Creating ANN Class with Needed Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e548809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, input_size):\n",
    "        # First hidden layer\n",
    "        self.w1 = 0.0099911 * np.random.randn(input_size, 32)\n",
    "        self.b1 = np.zeros((1, 32))\n",
    "        # Second hidden layer\n",
    "        self.w2 = 0.0099911 * np.random.randn(32, 16)\n",
    "        self.b2 = np.zeros((1, 16))\n",
    "        # Output layer\n",
    "        self.w3 = 0.0099911 * np.random.randn(16, 10)\n",
    "        self.b3 = np.zeros((1, 10))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # First hidden layer\n",
    "        z1 = np.dot(X, self.w1) + self.b1\n",
    "        a1 = np.maximum(0, z1)  # ReLU\n",
    "        # Second hidden layer\n",
    "        z2 = np.dot(a1, self.w2) + self.b2\n",
    "        a2 = np.maximum(0, z2)  # ReLU\n",
    "        # Output layer\n",
    "        z3 = np.dot(a2, self.w3) + self.b3\n",
    "        exp_z = np.exp(z3 - np.max(z3, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)  # Softmax\n",
    "    \n",
    "    def loss(self, pred, y):\n",
    "        return -np.mean(np.log(np.clip(pred[range(len(pred)), y], 1e-7, 1)))\n",
    "    \n",
    "    def save(self, name):\n",
    "        np.savez(name, w1=self.w1, b1=self.b1, w2=self.w2, b2=self.b2, w3=self.w3, b3=self.b3)\n",
    "    \n",
    "    def load(self, name):\n",
    "        data = np.load(name)\n",
    "        self.w1, self.b1 = data['w1'], data['b1']\n",
    "        self.w2, self.b2 = data['w2'], data['b2']\n",
    "        self.w3, self.b3 = data['w3'], data['b3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5fadd",
   "metadata": {},
   "source": [
    "# Creating Data Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c830824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    X, y = [], []\n",
    "    for digit in range(10):\n",
    "        folder = os.path.join(path, str(digit))\n",
    "        if os.path.exists(folder):\n",
    "            for file in os.listdir(folder):\n",
    "                if file.endswith(('.png')):\n",
    "                    img = Image.open(os.path.join(folder, file))\n",
    "                    arr = np.array(img, dtype=np.float64)\n",
    "                    flat_arr = arr.flatten()\n",
    "                    if flat_arr.max() > 1: flat_arr /= 255.0\n",
    "                    X.append(flat_arr)\n",
    "                    y.append(digit)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abaae2",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be552424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1980 images\n",
      "Image shape: (784,)\n",
      "Classes: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_data(\"Ai2_Dataset/Alaa\")\n",
    "print(f\"Loaded {len(X)} images\")\n",
    "print(f\"Image shape: {X[0].shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6351b",
   "metadata": {},
   "source": [
    "# **Creating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6b54d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with input size: 784\n",
      "Architecture: 784 -> 32 -> 16 -> 10\n"
     ]
    }
   ],
   "source": [
    "#Create model\n",
    "model = ANN(X.shape[1])\n",
    "print(f\"Model created with input size: {X.shape[1]}\")\n",
    "print(f\"Architecture: {X.shape[1]} -> 32 -> 16 -> 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df796d",
   "metadata": {},
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55486c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Step 0: Loss=2.303, Acc=0.100\n",
      "Step 6: Loss=2.303, Acc=0.100\n",
      "Step 7: Loss=2.302, Acc=0.100\n",
      "Step 11: Loss=2.302, Acc=0.100\n",
      "Step 17: Loss=2.302, Acc=0.100\n",
      "Step 31: Loss=2.302, Acc=0.141\n",
      "Step 42: Loss=2.302, Acc=0.121\n",
      "Step 98: Loss=2.301, Acc=0.105\n",
      "Step 99: Loss=2.301, Acc=0.099\n",
      "Step 101: Loss=2.301, Acc=0.156\n",
      "Step 102: Loss=2.301, Acc=0.101\n",
      "Step 106: Loss=2.301, Acc=0.100\n",
      "Step 109: Loss=2.300, Acc=0.174\n",
      "Step 112: Loss=2.300, Acc=0.151\n",
      "Step 117: Loss=2.300, Acc=0.139\n",
      "Step 122: Loss=2.299, Acc=0.171\n",
      "Step 128: Loss=2.298, Acc=0.117\n",
      "Step 138: Loss=2.298, Acc=0.106\n",
      "Step 139: Loss=2.298, Acc=0.104\n",
      "Step 142: Loss=2.297, Acc=0.105\n",
      "Step 145: Loss=2.296, Acc=0.107\n",
      "Step 146: Loss=2.294, Acc=0.145\n",
      "Step 147: Loss=2.292, Acc=0.187\n",
      "Step 148: Loss=2.291, Acc=0.134\n",
      "Step 156: Loss=2.291, Acc=0.156\n",
      "Step 157: Loss=2.289, Acc=0.100\n",
      "Step 158: Loss=2.289, Acc=0.140\n",
      "Step 161: Loss=2.289, Acc=0.102\n",
      "Step 165: Loss=2.288, Acc=0.102\n",
      "Step 177: Loss=2.288, Acc=0.102\n",
      "Step 180: Loss=2.287, Acc=0.129\n",
      "Step 181: Loss=2.287, Acc=0.130\n",
      "Step 183: Loss=2.287, Acc=0.156\n",
      "Step 186: Loss=2.287, Acc=0.149\n",
      "Step 187: Loss=2.285, Acc=0.134\n",
      "Step 188: Loss=2.285, Acc=0.100\n",
      "Step 196: Loss=2.285, Acc=0.100\n",
      "Step 197: Loss=2.284, Acc=0.123\n",
      "Step 198: Loss=2.281, Acc=0.121\n",
      "Step 200: Loss=2.280, Acc=0.213\n",
      "Step 202: Loss=2.279, Acc=0.202\n",
      "Step 207: Loss=2.279, Acc=0.110\n",
      "Step 208: Loss=2.277, Acc=0.103\n",
      "Step 211: Loss=2.274, Acc=0.137\n",
      "Step 215: Loss=2.271, Acc=0.239\n",
      "Step 219: Loss=2.268, Acc=0.230\n",
      "Step 233: Loss=2.268, Acc=0.124\n",
      "Step 243: Loss=2.266, Acc=0.124\n",
      "Step 246: Loss=2.265, Acc=0.113\n",
      "Step 250: Loss=2.264, Acc=0.123\n",
      "Step 252: Loss=2.263, Acc=0.161\n",
      "Step 255: Loss=2.261, Acc=0.162\n",
      "Step 258: Loss=2.258, Acc=0.133\n",
      "Step 262: Loss=2.257, Acc=0.180\n",
      "Step 269: Loss=2.257, Acc=0.160\n",
      "Step 273: Loss=2.255, Acc=0.159\n",
      "Step 279: Loss=2.255, Acc=0.186\n",
      "Step 281: Loss=2.253, Acc=0.189\n",
      "Step 283: Loss=2.252, Acc=0.189\n",
      "Step 300: Loss=2.250, Acc=0.191\n",
      "Step 302: Loss=2.245, Acc=0.217\n",
      "Step 303: Loss=2.245, Acc=0.212\n",
      "Step 307: Loss=2.244, Acc=0.224\n",
      "Step 310: Loss=2.230, Acc=0.237\n",
      "Step 311: Loss=2.226, Acc=0.265\n",
      "Step 321: Loss=2.220, Acc=0.239\n",
      "Step 325: Loss=2.214, Acc=0.253\n",
      "Step 331: Loss=2.212, Acc=0.230\n",
      "Step 342: Loss=2.210, Acc=0.212\n",
      "Step 346: Loss=2.205, Acc=0.264\n",
      "Step 349: Loss=2.204, Acc=0.254\n",
      "Step 353: Loss=2.202, Acc=0.210\n",
      "Step 358: Loss=2.201, Acc=0.238\n",
      "Step 359: Loss=2.199, Acc=0.244\n",
      "Step 362: Loss=2.198, Acc=0.238\n",
      "Step 382: Loss=2.195, Acc=0.247\n",
      "Step 385: Loss=2.192, Acc=0.248\n",
      "Step 393: Loss=2.190, Acc=0.291\n",
      "Step 395: Loss=2.190, Acc=0.290\n",
      "Step 398: Loss=2.186, Acc=0.273\n",
      "Step 399: Loss=2.186, Acc=0.275\n",
      "Step 403: Loss=2.183, Acc=0.290\n",
      "Step 407: Loss=2.178, Acc=0.301\n",
      "Step 410: Loss=2.177, Acc=0.286\n",
      "Step 411: Loss=2.175, Acc=0.292\n",
      "Step 412: Loss=2.172, Acc=0.316\n",
      "Step 427: Loss=2.171, Acc=0.331\n",
      "Step 431: Loss=2.170, Acc=0.283\n",
      "Step 434: Loss=2.169, Acc=0.250\n",
      "Step 435: Loss=2.168, Acc=0.297\n",
      "Step 445: Loss=2.167, Acc=0.270\n",
      "Step 448: Loss=2.167, Acc=0.292\n",
      "Step 450: Loss=2.163, Acc=0.277\n",
      "Step 451: Loss=2.162, Acc=0.283\n",
      "Step 455: Loss=2.160, Acc=0.305\n",
      "Step 457: Loss=2.160, Acc=0.272\n",
      "Step 459: Loss=2.155, Acc=0.293\n",
      "Step 461: Loss=2.154, Acc=0.299\n",
      "Step 462: Loss=2.152, Acc=0.325\n",
      "Step 473: Loss=2.150, Acc=0.340\n",
      "Step 481: Loss=2.150, Acc=0.321\n",
      "Step 483: Loss=2.147, Acc=0.331\n",
      "Step 485: Loss=2.142, Acc=0.331\n",
      "Step 487: Loss=2.142, Acc=0.330\n",
      "Step 489: Loss=2.141, Acc=0.336\n",
      "Step 491: Loss=2.138, Acc=0.326\n",
      "Step 493: Loss=2.136, Acc=0.318\n",
      "Step 495: Loss=2.120, Acc=0.364\n",
      "Step 504: Loss=2.118, Acc=0.337\n",
      "Step 506: Loss=2.111, Acc=0.371\n",
      "Step 509: Loss=2.109, Acc=0.333\n",
      "Step 540: Loss=2.102, Acc=0.366\n",
      "Step 542: Loss=2.100, Acc=0.393\n",
      "Step 556: Loss=2.097, Acc=0.403\n",
      "Step 558: Loss=2.096, Acc=0.372\n",
      "Step 560: Loss=2.092, Acc=0.362\n",
      "Step 568: Loss=2.084, Acc=0.367\n",
      "Step 577: Loss=2.082, Acc=0.409\n",
      "Step 578: Loss=2.081, Acc=0.384\n",
      "Step 580: Loss=2.079, Acc=0.385\n",
      "Step 582: Loss=2.076, Acc=0.353\n",
      "Step 583: Loss=2.071, Acc=0.370\n",
      "Step 586: Loss=2.066, Acc=0.349\n",
      "Step 589: Loss=2.060, Acc=0.313\n",
      "Step 590: Loss=2.057, Acc=0.319\n",
      "Step 591: Loss=2.043, Acc=0.358\n",
      "Step 593: Loss=2.038, Acc=0.367\n",
      "Step 595: Loss=2.022, Acc=0.413\n",
      "Step 607: Loss=2.015, Acc=0.406\n",
      "Step 613: Loss=2.004, Acc=0.412\n",
      "Step 624: Loss=2.000, Acc=0.403\n",
      "Step 632: Loss=1.999, Acc=0.420\n",
      "Step 640: Loss=1.995, Acc=0.427\n",
      "Step 643: Loss=1.992, Acc=0.400\n",
      "Step 650: Loss=1.985, Acc=0.379\n",
      "Step 653: Loss=1.978, Acc=0.396\n",
      "Step 657: Loss=1.978, Acc=0.430\n",
      "Step 660: Loss=1.969, Acc=0.427\n",
      "Step 678: Loss=1.966, Acc=0.424\n",
      "Step 679: Loss=1.966, Acc=0.430\n",
      "Step 683: Loss=1.959, Acc=0.421\n",
      "Step 686: Loss=1.949, Acc=0.448\n",
      "Step 689: Loss=1.945, Acc=0.451\n",
      "Step 691: Loss=1.943, Acc=0.471\n",
      "Step 698: Loss=1.939, Acc=0.476\n",
      "Step 705: Loss=1.937, Acc=0.467\n",
      "Step 716: Loss=1.931, Acc=0.434\n",
      "Step 727: Loss=1.924, Acc=0.430\n",
      "Step 734: Loss=1.923, Acc=0.440\n",
      "Step 743: Loss=1.923, Acc=0.471\n",
      "Step 747: Loss=1.921, Acc=0.449\n",
      "Step 752: Loss=1.912, Acc=0.466\n",
      "Step 759: Loss=1.910, Acc=0.480\n",
      "Step 776: Loss=1.899, Acc=0.463\n",
      "Step 778: Loss=1.897, Acc=0.469\n",
      "Step 788: Loss=1.895, Acc=0.445\n",
      "Step 804: Loss=1.891, Acc=0.452\n",
      "Step 810: Loss=1.890, Acc=0.441\n",
      "Step 816: Loss=1.889, Acc=0.415\n",
      "Step 824: Loss=1.886, Acc=0.417\n",
      "Step 825: Loss=1.883, Acc=0.411\n",
      "Step 831: Loss=1.868, Acc=0.436\n",
      "Step 844: Loss=1.860, Acc=0.477\n",
      "Step 846: Loss=1.848, Acc=0.456\n",
      "Step 865: Loss=1.847, Acc=0.433\n",
      "Step 866: Loss=1.847, Acc=0.423\n",
      "Step 895: Loss=1.844, Acc=0.415\n",
      "Step 919: Loss=1.842, Acc=0.401\n",
      "Step 926: Loss=1.837, Acc=0.411\n",
      "Step 932: Loss=1.836, Acc=0.420\n",
      "Step 937: Loss=1.828, Acc=0.433\n",
      "Step 944: Loss=1.819, Acc=0.431\n",
      "Step 952: Loss=1.808, Acc=0.454\n",
      "Step 960: Loss=1.792, Acc=0.425\n",
      "Step 969: Loss=1.783, Acc=0.451\n",
      "Step 975: Loss=1.780, Acc=0.432\n",
      "Step 985: Loss=1.779, Acc=0.482\n",
      "Step 988: Loss=1.762, Acc=0.502\n",
      "Step 992: Loss=1.745, Acc=0.528\n",
      "Step 1024: Loss=1.744, Acc=0.495\n",
      "Step 1029: Loss=1.738, Acc=0.501\n",
      "Step 1034: Loss=1.736, Acc=0.482\n",
      "Step 1046: Loss=1.731, Acc=0.481\n",
      "Step 1060: Loss=1.725, Acc=0.473\n",
      "Step 1068: Loss=1.716, Acc=0.489\n",
      "Step 1069: Loss=1.716, Acc=0.475\n",
      "Step 1076: Loss=1.714, Acc=0.483\n",
      "Step 1078: Loss=1.710, Acc=0.494\n",
      "Step 1080: Loss=1.703, Acc=0.481\n",
      "Step 1081: Loss=1.698, Acc=0.504\n",
      "Step 1092: Loss=1.685, Acc=0.531\n",
      "Step 1097: Loss=1.685, Acc=0.517\n",
      "Step 1103: Loss=1.677, Acc=0.477\n",
      "Step 1107: Loss=1.670, Acc=0.475\n",
      "Step 1114: Loss=1.650, Acc=0.538\n",
      "Step 1115: Loss=1.647, Acc=0.545\n",
      "Step 1123: Loss=1.646, Acc=0.498\n",
      "Step 1133: Loss=1.638, Acc=0.471\n",
      "Step 1134: Loss=1.631, Acc=0.490\n",
      "Step 1154: Loss=1.627, Acc=0.519\n",
      "Step 1161: Loss=1.625, Acc=0.561\n",
      "Step 1171: Loss=1.617, Acc=0.545\n",
      "Step 1197: Loss=1.616, Acc=0.598\n",
      "Step 1206: Loss=1.616, Acc=0.574\n",
      "Step 1251: Loss=1.608, Acc=0.579\n",
      "Step 1259: Loss=1.607, Acc=0.562\n",
      "Step 1265: Loss=1.600, Acc=0.564\n",
      "Step 1266: Loss=1.597, Acc=0.530\n",
      "Step 1267: Loss=1.596, Acc=0.561\n",
      "Step 1268: Loss=1.586, Acc=0.572\n",
      "Step 1272: Loss=1.583, Acc=0.562\n",
      "Step 1306: Loss=1.578, Acc=0.560\n",
      "Step 1311: Loss=1.554, Acc=0.558\n",
      "Step 1332: Loss=1.538, Acc=0.602\n",
      "Step 1333: Loss=1.538, Acc=0.589\n",
      "Step 1350: Loss=1.532, Acc=0.576\n",
      "Step 1352: Loss=1.531, Acc=0.563\n",
      "Step 1354: Loss=1.527, Acc=0.573\n",
      "Step 1358: Loss=1.526, Acc=0.573\n",
      "Step 1362: Loss=1.519, Acc=0.585\n",
      "Step 1377: Loss=1.515, Acc=0.568\n",
      "Step 1404: Loss=1.512, Acc=0.556\n",
      "Step 1411: Loss=1.511, Acc=0.558\n",
      "Step 1414: Loss=1.489, Acc=0.582\n",
      "Step 1417: Loss=1.477, Acc=0.564\n",
      "Step 1426: Loss=1.461, Acc=0.572\n",
      "Step 1467: Loss=1.453, Acc=0.564\n",
      "Step 1471: Loss=1.449, Acc=0.570\n",
      "Step 1476: Loss=1.447, Acc=0.587\n",
      "Step 1478: Loss=1.438, Acc=0.593\n",
      "Step 1481: Loss=1.436, Acc=0.597\n",
      "Step 1523: Loss=1.433, Acc=0.590\n",
      "Step 1548: Loss=1.427, Acc=0.621\n",
      "Step 1592: Loss=1.423, Acc=0.624\n",
      "Step 1595: Loss=1.418, Acc=0.636\n",
      "Step 1598: Loss=1.417, Acc=0.610\n",
      "Step 1610: Loss=1.416, Acc=0.616\n",
      "Step 1674: Loss=1.411, Acc=0.622\n",
      "Step 1698: Loss=1.402, Acc=0.645\n",
      "Step 1708: Loss=1.402, Acc=0.625\n",
      "Step 1717: Loss=1.402, Acc=0.626\n",
      "Step 1735: Loss=1.395, Acc=0.644\n",
      "Step 1751: Loss=1.383, Acc=0.629\n",
      "Step 1757: Loss=1.382, Acc=0.628\n",
      "Step 1784: Loss=1.381, Acc=0.629\n",
      "Step 1793: Loss=1.371, Acc=0.616\n",
      "Step 1809: Loss=1.365, Acc=0.605\n",
      "Step 1812: Loss=1.362, Acc=0.611\n",
      "Step 1816: Loss=1.356, Acc=0.613\n",
      "Step 1857: Loss=1.356, Acc=0.600\n",
      "Step 1859: Loss=1.351, Acc=0.597\n",
      "Step 1871: Loss=1.350, Acc=0.598\n",
      "Step 1874: Loss=1.344, Acc=0.612\n",
      "Step 1888: Loss=1.337, Acc=0.628\n",
      "Step 1896: Loss=1.318, Acc=0.658\n",
      "Step 1908: Loss=1.308, Acc=0.657\n",
      "Step 1919: Loss=1.308, Acc=0.648\n",
      "Step 1926: Loss=1.302, Acc=0.639\n",
      "Step 1927: Loss=1.299, Acc=0.659\n",
      "Step 1936: Loss=1.297, Acc=0.665\n",
      "Step 1960: Loss=1.294, Acc=0.657\n",
      "Step 1987: Loss=1.292, Acc=0.677\n",
      "Step 2011: Loss=1.281, Acc=0.686\n",
      "Step 2027: Loss=1.275, Acc=0.670\n",
      "Step 2133: Loss=1.269, Acc=0.683\n",
      "Step 2134: Loss=1.269, Acc=0.684\n",
      "Step 2153: Loss=1.266, Acc=0.694\n",
      "Step 2156: Loss=1.261, Acc=0.702\n",
      "Step 2171: Loss=1.253, Acc=0.698\n",
      "Step 2175: Loss=1.239, Acc=0.699\n",
      "Step 2187: Loss=1.237, Acc=0.696\n",
      "Step 2195: Loss=1.226, Acc=0.679\n",
      "Step 2224: Loss=1.223, Acc=0.677\n",
      "Step 2225: Loss=1.218, Acc=0.675\n",
      "Step 2230: Loss=1.214, Acc=0.668\n",
      "Step 2248: Loss=1.212, Acc=0.645\n",
      "Step 2301: Loss=1.193, Acc=0.650\n",
      "Step 2320: Loss=1.192, Acc=0.670\n",
      "Step 2328: Loss=1.189, Acc=0.672\n",
      "Step 2350: Loss=1.187, Acc=0.685\n",
      "Step 2407: Loss=1.185, Acc=0.676\n",
      "Step 2410: Loss=1.184, Acc=0.645\n",
      "Step 2421: Loss=1.177, Acc=0.657\n",
      "Step 2439: Loss=1.174, Acc=0.666\n",
      "Step 2447: Loss=1.173, Acc=0.666\n",
      "Step 2465: Loss=1.168, Acc=0.654\n",
      "Step 2470: Loss=1.162, Acc=0.669\n",
      "Step 2544: Loss=1.154, Acc=0.667\n",
      "Step 2567: Loss=1.152, Acc=0.665\n",
      "Step 2608: Loss=1.149, Acc=0.682\n",
      "Step 2659: Loss=1.143, Acc=0.677\n",
      "Step 2661: Loss=1.139, Acc=0.696\n",
      "Step 2684: Loss=1.134, Acc=0.682\n",
      "Step 2685: Loss=1.130, Acc=0.685\n",
      "Step 2693: Loss=1.123, Acc=0.686\n",
      "Step 2701: Loss=1.104, Acc=0.675\n",
      "Step 2704: Loss=1.101, Acc=0.699\n",
      "Step 2707: Loss=1.101, Acc=0.688\n",
      "Step 2725: Loss=1.099, Acc=0.680\n",
      "Step 2753: Loss=1.095, Acc=0.696\n",
      "Step 2754: Loss=1.091, Acc=0.680\n",
      "Step 2758: Loss=1.087, Acc=0.668\n",
      "Step 2771: Loss=1.080, Acc=0.666\n",
      "Step 2838: Loss=1.079, Acc=0.655\n",
      "Step 2841: Loss=1.072, Acc=0.667\n",
      "Step 2842: Loss=1.069, Acc=0.669\n",
      "Step 2844: Loss=1.057, Acc=0.683\n",
      "Step 2873: Loss=1.053, Acc=0.685\n",
      "Step 2886: Loss=1.051, Acc=0.658\n",
      "Step 2887: Loss=1.045, Acc=0.692\n",
      "Step 2891: Loss=1.043, Acc=0.693\n",
      "Step 2909: Loss=1.042, Acc=0.689\n",
      "Step 2924: Loss=1.035, Acc=0.688\n",
      "Step 2955: Loss=1.022, Acc=0.709\n",
      "Step 2967: Loss=1.020, Acc=0.708\n",
      "Step 3188: Loss=1.015, Acc=0.715\n",
      "Step 3220: Loss=1.014, Acc=0.707\n",
      "Step 3276: Loss=1.008, Acc=0.727\n",
      "Step 3305: Loss=1.006, Acc=0.723\n",
      "Step 3314: Loss=1.003, Acc=0.719\n",
      "Step 3323: Loss=1.002, Acc=0.715\n",
      "Step 3403: Loss=1.002, Acc=0.713\n",
      "Step 3429: Loss=1.000, Acc=0.711\n",
      "Step 3430: Loss=0.987, Acc=0.733\n",
      "Step 3438: Loss=0.984, Acc=0.723\n",
      "Step 3449: Loss=0.972, Acc=0.733\n",
      "Step 3523: Loss=0.966, Acc=0.728\n",
      "Step 3526: Loss=0.945, Acc=0.749\n",
      "Step 3568: Loss=0.934, Acc=0.744\n",
      "Step 3952: Loss=0.929, Acc=0.752\n",
      "Step 3957: Loss=0.926, Acc=0.748\n",
      "Step 4012: Loss=0.926, Acc=0.754\n",
      "Step 4042: Loss=0.919, Acc=0.756\n",
      "Step 4045: Loss=0.911, Acc=0.759\n",
      "Step 4058: Loss=0.907, Acc=0.759\n",
      "Step 4149: Loss=0.898, Acc=0.757\n",
      "Step 4307: Loss=0.896, Acc=0.760\n",
      "Step 4359: Loss=0.893, Acc=0.776\n",
      "Step 4415: Loss=0.893, Acc=0.768\n",
      "Step 4429: Loss=0.892, Acc=0.764\n",
      "Step 4448: Loss=0.886, Acc=0.763\n",
      "Step 4462: Loss=0.884, Acc=0.765\n",
      "Step 4516: Loss=0.878, Acc=0.775\n",
      "Step 4522: Loss=0.877, Acc=0.768\n",
      "Step 4646: Loss=0.873, Acc=0.781\n",
      "Step 4650: Loss=0.869, Acc=0.774\n",
      "Step 4652: Loss=0.865, Acc=0.766\n",
      "Step 4654: Loss=0.865, Acc=0.761\n",
      "Step 4668: Loss=0.861, Acc=0.763\n",
      "Step 4695: Loss=0.859, Acc=0.754\n",
      "Step 4701: Loss=0.857, Acc=0.773\n",
      "Step 4703: Loss=0.857, Acc=0.772\n",
      "Step 4717: Loss=0.851, Acc=0.773\n",
      "Step 4730: Loss=0.850, Acc=0.766\n",
      "Step 4752: Loss=0.847, Acc=0.755\n",
      "Step 4769: Loss=0.837, Acc=0.768\n",
      "Step 4829: Loss=0.833, Acc=0.770\n",
      "Step 4872: Loss=0.831, Acc=0.768\n",
      "Step 4893: Loss=0.825, Acc=0.781\n",
      "Step 4895: Loss=0.817, Acc=0.766\n",
      "Step 4916: Loss=0.813, Acc=0.772\n",
      "Step 4933: Loss=0.808, Acc=0.770\n",
      "Step 4935: Loss=0.803, Acc=0.774\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "print(\"Starting training...\")\n",
    "best_loss = 999\n",
    "for i in range(5000):\n",
    "    # Random weight updates for all layers\n",
    "    model.w1 += 0.0099911 * np.random.randn(*model.w1.shape)\n",
    "    model.b1 += 0.0099911 * np.random.randn(*model.b1.shape)\n",
    "    model.w2 += 0.0099911 * np.random.randn(*model.w2.shape)\n",
    "    model.b2 += 0.0099911 * np.random.randn(*model.b2.shape)\n",
    "    model.w3 += 0.0099911 * np.random.randn(*model.w3.shape)\n",
    "    model.b3 += 0.0099911 * np.random.randn(*model.b3.shape)\n",
    "    \n",
    "    pred = model.forward(X)\n",
    "    loss = model.loss(pred, y)\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        model.save(\"Alaa2235858Model.npz\")\n",
    "        acc = np.mean(np.argmax(pred, axis=1) == y)\n",
    "        print(f\"Step {i}: Loss={loss:.3f}, Acc={acc:.3f}\")\n",
    "    else:\n",
    "        model.load(\"Alaa2235858Model.npz\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac193741",
   "metadata": {},
   "source": [
    "# **Loading Result Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c10f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(\"Alaa2235858Model.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccb841",
   "metadata": {},
   "source": [
    "# **Test on shared test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3239585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shared test set...\n",
      "Shared Test Accuracy: 0.690\n"
     ]
    }
   ],
   "source": [
    "test_path = \"Ai2_Dataset/SharedTesting\"\n",
    "if os.path.exists(test_path):\n",
    "    print(\"Loading shared test set...\")\n",
    "    test_X, test_y = load_data(test_path)\n",
    "    test_pred = model.forward(test_X)\n",
    "    test_acc = np.mean(np.argmax(test_pred, axis=1) == test_y)\n",
    "    print(f\"Shared Test Accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfccc61d",
   "metadata": {},
   "source": [
    "# **Print every prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8cba0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Predicted=0, Actual=0\n",
      "Sample 1: Predicted=0, Actual=0\n",
      "Sample 2: Predicted=0, Actual=0\n",
      "Sample 3: Predicted=0, Actual=0\n",
      "Sample 4: Predicted=9, Actual=0\n",
      "Sample 5: Predicted=0, Actual=0\n",
      "Sample 6: Predicted=0, Actual=0\n",
      "Sample 7: Predicted=0, Actual=0\n",
      "Sample 8: Predicted=0, Actual=0\n",
      "Sample 9: Predicted=0, Actual=0\n",
      "Sample 10: Predicted=1, Actual=1\n",
      "Sample 11: Predicted=1, Actual=1\n",
      "Sample 12: Predicted=1, Actual=1\n",
      "Sample 13: Predicted=1, Actual=1\n",
      "Sample 14: Predicted=1, Actual=1\n",
      "Sample 15: Predicted=1, Actual=1\n",
      "Sample 16: Predicted=1, Actual=1\n",
      "Sample 17: Predicted=1, Actual=1\n",
      "Sample 18: Predicted=1, Actual=1\n",
      "Sample 19: Predicted=1, Actual=1\n",
      "Sample 20: Predicted=3, Actual=2\n",
      "Sample 21: Predicted=3, Actual=2\n",
      "Sample 22: Predicted=2, Actual=2\n",
      "Sample 23: Predicted=2, Actual=2\n",
      "Sample 24: Predicted=2, Actual=2\n",
      "Sample 25: Predicted=5, Actual=2\n",
      "Sample 26: Predicted=2, Actual=2\n",
      "Sample 27: Predicted=3, Actual=2\n",
      "Sample 28: Predicted=4, Actual=2\n",
      "Sample 29: Predicted=3, Actual=2\n",
      "Sample 30: Predicted=3, Actual=3\n",
      "Sample 31: Predicted=3, Actual=3\n",
      "Sample 32: Predicted=1, Actual=3\n",
      "Sample 33: Predicted=3, Actual=3\n",
      "Sample 34: Predicted=3, Actual=3\n",
      "Sample 35: Predicted=3, Actual=3\n",
      "Sample 36: Predicted=3, Actual=3\n",
      "Sample 37: Predicted=7, Actual=3\n",
      "Sample 38: Predicted=3, Actual=3\n",
      "Sample 39: Predicted=3, Actual=3\n",
      "Sample 40: Predicted=4, Actual=4\n",
      "Sample 41: Predicted=4, Actual=4\n",
      "Sample 42: Predicted=4, Actual=4\n",
      "Sample 43: Predicted=1, Actual=4\n",
      "Sample 44: Predicted=4, Actual=4\n",
      "Sample 45: Predicted=1, Actual=4\n",
      "Sample 46: Predicted=4, Actual=4\n",
      "Sample 47: Predicted=2, Actual=4\n",
      "Sample 48: Predicted=2, Actual=4\n",
      "Sample 49: Predicted=5, Actual=4\n",
      "Sample 50: Predicted=5, Actual=5\n",
      "Sample 51: Predicted=5, Actual=5\n",
      "Sample 52: Predicted=5, Actual=5\n",
      "Sample 53: Predicted=5, Actual=5\n",
      "Sample 54: Predicted=5, Actual=5\n",
      "Sample 55: Predicted=5, Actual=5\n",
      "Sample 56: Predicted=5, Actual=5\n",
      "Sample 57: Predicted=5, Actual=5\n",
      "Sample 58: Predicted=6, Actual=5\n",
      "Sample 59: Predicted=5, Actual=5\n",
      "Sample 60: Predicted=6, Actual=6\n",
      "Sample 61: Predicted=6, Actual=6\n",
      "Sample 62: Predicted=9, Actual=6\n",
      "Sample 63: Predicted=6, Actual=6\n",
      "Sample 64: Predicted=6, Actual=6\n",
      "Sample 65: Predicted=8, Actual=6\n",
      "Sample 66: Predicted=9, Actual=6\n",
      "Sample 67: Predicted=8, Actual=6\n",
      "Sample 68: Predicted=9, Actual=6\n",
      "Sample 69: Predicted=6, Actual=6\n",
      "Sample 70: Predicted=4, Actual=7\n",
      "Sample 71: Predicted=7, Actual=7\n",
      "Sample 72: Predicted=7, Actual=7\n",
      "Sample 73: Predicted=5, Actual=7\n",
      "Sample 74: Predicted=7, Actual=7\n",
      "Sample 75: Predicted=7, Actual=7\n",
      "Sample 76: Predicted=7, Actual=7\n",
      "Sample 77: Predicted=9, Actual=7\n",
      "Sample 78: Predicted=0, Actual=7\n",
      "Sample 79: Predicted=7, Actual=7\n",
      "Sample 80: Predicted=8, Actual=8\n",
      "Sample 81: Predicted=9, Actual=8\n",
      "Sample 82: Predicted=4, Actual=8\n",
      "Sample 83: Predicted=8, Actual=8\n",
      "Sample 84: Predicted=9, Actual=8\n",
      "Sample 85: Predicted=5, Actual=8\n",
      "Sample 86: Predicted=6, Actual=8\n",
      "Sample 87: Predicted=8, Actual=8\n",
      "Sample 88: Predicted=6, Actual=8\n",
      "Sample 89: Predicted=8, Actual=8\n",
      "Sample 90: Predicted=9, Actual=9\n",
      "Sample 91: Predicted=9, Actual=9\n",
      "Sample 92: Predicted=9, Actual=9\n",
      "Sample 93: Predicted=9, Actual=9\n",
      "Sample 94: Predicted=9, Actual=9\n",
      "Sample 95: Predicted=9, Actual=9\n",
      "Sample 96: Predicted=4, Actual=9\n",
      "Sample 97: Predicted=9, Actual=9\n",
      "Sample 98: Predicted=9, Actual=9\n",
      "Sample 99: Predicted=9, Actual=9\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for i, pred in enumerate(test_pred):\n",
    "    predicted_class = np.argmax(pred)\n",
    "    actual_class = test_y[i]\n",
    "    print(f\"Sample {i}: Predicted={predicted_class}, Actual={actual_class}\")\n",
    "else:\n",
    "    print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
