{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa4a95b",
   "metadata": {},
   "source": [
    "#### Name: Turki Alqahtani\n",
    "#### ID: 2236938"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3601b",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4920fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fd458",
   "metadata": {},
   "source": [
    "# Set My Own Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b1dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd0213",
   "metadata": {},
   "source": [
    "# Creating ANN Class with Needed Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e548809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, input_size):\n",
    "        # First hidden layer\n",
    "        self.w1 = 0.01 * np.random.randn(input_size, 32)\n",
    "        self.b1 = np.zeros((1, 32))\n",
    "        # Second hidden layer\n",
    "        self.w2 = 0.01 * np.random.randn(32, 16)\n",
    "        self.b2 = np.zeros((1, 16))\n",
    "        # Output layer\n",
    "        self.w3 = 0.01 * np.random.randn(16, 10)\n",
    "        self.b3 = np.zeros((1, 10))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # First hidden layer\n",
    "        z1 = np.dot(X, self.w1) + self.b1\n",
    "        a1 = np.maximum(0, z1)  # ReLU\n",
    "        # Second hidden layer\n",
    "        z2 = np.dot(a1, self.w2) + self.b2\n",
    "        a2 = np.maximum(0, z2)  # ReLU\n",
    "        # Output layer\n",
    "        z3 = np.dot(a2, self.w3) + self.b3\n",
    "        exp_z = np.exp(z3 - np.max(z3, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)  # Softmax\n",
    "    \n",
    "    def loss(self, pred, y):\n",
    "        return -np.mean(np.log(np.clip(pred[range(len(pred)), y], 1e-7, 1)))\n",
    "    \n",
    "    def save(self, name):\n",
    "        np.savez(name, w1=self.w1, b1=self.b1, w2=self.w2, b2=self.b2, w3=self.w3, b3=self.b3)\n",
    "    \n",
    "    def load(self, name):\n",
    "        data = np.load(name)\n",
    "        self.w1, self.b1 = data['w1'], data['b1']\n",
    "        self.w2, self.b2 = data['w2'], data['b2']\n",
    "        self.w3, self.b3 = data['w3'], data['b3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5fadd",
   "metadata": {},
   "source": [
    "# Creating Data Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c830824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    X, y = [], []\n",
    "    for digit in range(10):\n",
    "        folder = os.path.join(path, str(digit))\n",
    "        if os.path.exists(folder):\n",
    "            for file in os.listdir(folder):\n",
    "                if file.endswith(('.png')):\n",
    "                    img = Image.open(os.path.join(folder, file))\n",
    "                    arr = np.array(img, dtype=np.float64)\n",
    "                    flat_arr = arr.flatten()\n",
    "                    if flat_arr.max() > 1: flat_arr /= 255.0\n",
    "                    X.append(flat_arr)\n",
    "                    y.append(digit)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abaae2",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be552424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1980 images\n",
      "Image shape: (784,)\n",
      "Classes: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_data(\"Ai2_Dataset/Turki Q\")\n",
    "print(f\"Loaded {len(X)} images\")\n",
    "print(f\"Image shape: {X[0].shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6351b",
   "metadata": {},
   "source": [
    "# **Creating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6b54d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with input size: 784\n",
      "Architecture: 784 -> 32 -> 16 -> 10\n"
     ]
    }
   ],
   "source": [
    "#Create model\n",
    "model = ANN(X.shape[1])\n",
    "print(f\"Model created with input size: {X.shape[1]}\")\n",
    "print(f\"Architecture: {X.shape[1]} -> 32 -> 16 -> 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df796d",
   "metadata": {},
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55486c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Step 0: Loss=2.303, Acc=0.099\n",
      "Step 2: Loss=2.303, Acc=0.104\n",
      "Step 4: Loss=2.303, Acc=0.100\n",
      "Step 7: Loss=2.303, Acc=0.100\n",
      "Step 10: Loss=2.303, Acc=0.088\n",
      "Step 17: Loss=2.303, Acc=0.100\n",
      "Step 19: Loss=2.302, Acc=0.100\n",
      "Step 21: Loss=2.302, Acc=0.100\n",
      "Step 26: Loss=2.302, Acc=0.100\n",
      "Step 27: Loss=2.302, Acc=0.100\n",
      "Step 30: Loss=2.302, Acc=0.100\n",
      "Step 37: Loss=2.302, Acc=0.100\n",
      "Step 40: Loss=2.302, Acc=0.100\n",
      "Step 41: Loss=2.302, Acc=0.100\n",
      "Step 42: Loss=2.301, Acc=0.100\n",
      "Step 46: Loss=2.300, Acc=0.100\n",
      "Step 48: Loss=2.300, Acc=0.192\n",
      "Step 51: Loss=2.300, Acc=0.192\n",
      "Step 52: Loss=2.300, Acc=0.178\n",
      "Step 53: Loss=2.299, Acc=0.129\n",
      "Step 57: Loss=2.299, Acc=0.211\n",
      "Step 59: Loss=2.298, Acc=0.206\n",
      "Step 62: Loss=2.298, Acc=0.177\n",
      "Step 65: Loss=2.298, Acc=0.171\n",
      "Step 70: Loss=2.297, Acc=0.179\n",
      "Step 71: Loss=2.297, Acc=0.164\n",
      "Step 72: Loss=2.296, Acc=0.178\n",
      "Step 77: Loss=2.295, Acc=0.109\n",
      "Step 79: Loss=2.295, Acc=0.096\n",
      "Step 80: Loss=2.294, Acc=0.143\n",
      "Step 81: Loss=2.294, Acc=0.100\n",
      "Step 83: Loss=2.293, Acc=0.119\n",
      "Step 85: Loss=2.292, Acc=0.213\n",
      "Step 88: Loss=2.292, Acc=0.209\n",
      "Step 90: Loss=2.291, Acc=0.126\n",
      "Step 97: Loss=2.290, Acc=0.196\n",
      "Step 98: Loss=2.290, Acc=0.213\n",
      "Step 100: Loss=2.289, Acc=0.197\n",
      "Step 105: Loss=2.289, Acc=0.242\n",
      "Step 108: Loss=2.288, Acc=0.229\n",
      "Step 111: Loss=2.287, Acc=0.214\n",
      "Step 115: Loss=2.286, Acc=0.117\n",
      "Step 118: Loss=2.285, Acc=0.134\n",
      "Step 126: Loss=2.285, Acc=0.142\n",
      "Step 128: Loss=2.283, Acc=0.137\n",
      "Step 147: Loss=2.282, Acc=0.142\n",
      "Step 150: Loss=2.282, Acc=0.141\n",
      "Step 154: Loss=2.280, Acc=0.134\n",
      "Step 157: Loss=2.280, Acc=0.207\n",
      "Step 158: Loss=2.279, Acc=0.185\n",
      "Step 164: Loss=2.279, Acc=0.172\n",
      "Step 165: Loss=2.277, Acc=0.132\n",
      "Step 170: Loss=2.274, Acc=0.221\n",
      "Step 181: Loss=2.274, Acc=0.172\n",
      "Step 184: Loss=2.273, Acc=0.187\n",
      "Step 188: Loss=2.269, Acc=0.152\n",
      "Step 195: Loss=2.267, Acc=0.145\n",
      "Step 196: Loss=2.262, Acc=0.183\n",
      "Step 206: Loss=2.262, Acc=0.231\n",
      "Step 210: Loss=2.261, Acc=0.236\n",
      "Step 213: Loss=2.261, Acc=0.197\n",
      "Step 216: Loss=2.260, Acc=0.170\n",
      "Step 225: Loss=2.259, Acc=0.191\n",
      "Step 226: Loss=2.257, Acc=0.229\n",
      "Step 228: Loss=2.255, Acc=0.284\n",
      "Step 230: Loss=2.253, Acc=0.306\n",
      "Step 235: Loss=2.253, Acc=0.269\n",
      "Step 238: Loss=2.252, Acc=0.252\n",
      "Step 245: Loss=2.251, Acc=0.268\n",
      "Step 249: Loss=2.251, Acc=0.265\n",
      "Step 251: Loss=2.250, Acc=0.255\n",
      "Step 261: Loss=2.246, Acc=0.254\n",
      "Step 266: Loss=2.246, Acc=0.173\n",
      "Step 267: Loss=2.244, Acc=0.146\n",
      "Step 270: Loss=2.244, Acc=0.138\n",
      "Step 273: Loss=2.241, Acc=0.140\n",
      "Step 274: Loss=2.238, Acc=0.156\n",
      "Step 279: Loss=2.235, Acc=0.170\n",
      "Step 290: Loss=2.226, Acc=0.182\n",
      "Step 302: Loss=2.224, Acc=0.206\n",
      "Step 308: Loss=2.220, Acc=0.171\n",
      "Step 310: Loss=2.220, Acc=0.170\n",
      "Step 313: Loss=2.213, Acc=0.209\n",
      "Step 316: Loss=2.209, Acc=0.242\n",
      "Step 319: Loss=2.206, Acc=0.264\n",
      "Step 323: Loss=2.204, Acc=0.240\n",
      "Step 325: Loss=2.202, Acc=0.247\n",
      "Step 328: Loss=2.200, Acc=0.231\n",
      "Step 334: Loss=2.199, Acc=0.242\n",
      "Step 336: Loss=2.197, Acc=0.219\n",
      "Step 341: Loss=2.195, Acc=0.198\n",
      "Step 342: Loss=2.191, Acc=0.233\n",
      "Step 343: Loss=2.190, Acc=0.231\n",
      "Step 346: Loss=2.190, Acc=0.221\n",
      "Step 347: Loss=2.187, Acc=0.194\n",
      "Step 348: Loss=2.183, Acc=0.216\n",
      "Step 351: Loss=2.182, Acc=0.231\n",
      "Step 355: Loss=2.180, Acc=0.232\n",
      "Step 361: Loss=2.178, Acc=0.244\n",
      "Step 363: Loss=2.173, Acc=0.242\n",
      "Step 369: Loss=2.166, Acc=0.214\n",
      "Step 372: Loss=2.165, Acc=0.216\n",
      "Step 373: Loss=2.164, Acc=0.228\n",
      "Step 381: Loss=2.154, Acc=0.235\n",
      "Step 393: Loss=2.152, Acc=0.238\n",
      "Step 398: Loss=2.151, Acc=0.258\n",
      "Step 402: Loss=2.146, Acc=0.273\n",
      "Step 408: Loss=2.142, Acc=0.244\n",
      "Step 412: Loss=2.141, Acc=0.248\n",
      "Step 415: Loss=2.137, Acc=0.243\n",
      "Step 446: Loss=2.132, Acc=0.257\n",
      "Step 450: Loss=2.126, Acc=0.249\n",
      "Step 453: Loss=2.125, Acc=0.228\n",
      "Step 454: Loss=2.114, Acc=0.245\n",
      "Step 455: Loss=2.109, Acc=0.274\n",
      "Step 461: Loss=2.102, Acc=0.296\n",
      "Step 464: Loss=2.097, Acc=0.297\n",
      "Step 469: Loss=2.095, Acc=0.247\n",
      "Step 478: Loss=2.093, Acc=0.252\n",
      "Step 481: Loss=2.090, Acc=0.244\n",
      "Step 482: Loss=2.088, Acc=0.239\n",
      "Step 487: Loss=2.081, Acc=0.247\n",
      "Step 496: Loss=2.077, Acc=0.251\n",
      "Step 500: Loss=2.074, Acc=0.247\n",
      "Step 505: Loss=2.071, Acc=0.282\n",
      "Step 506: Loss=2.063, Acc=0.302\n",
      "Step 521: Loss=2.055, Acc=0.304\n",
      "Step 534: Loss=2.053, Acc=0.280\n",
      "Step 544: Loss=2.051, Acc=0.299\n",
      "Step 545: Loss=2.049, Acc=0.285\n",
      "Step 556: Loss=2.048, Acc=0.276\n",
      "Step 557: Loss=2.045, Acc=0.296\n",
      "Step 558: Loss=2.042, Acc=0.296\n",
      "Step 581: Loss=2.037, Acc=0.320\n",
      "Step 592: Loss=2.036, Acc=0.315\n",
      "Step 599: Loss=2.026, Acc=0.313\n",
      "Step 602: Loss=2.018, Acc=0.315\n",
      "Step 625: Loss=2.006, Acc=0.328\n",
      "Step 629: Loss=1.999, Acc=0.345\n",
      "Step 641: Loss=1.998, Acc=0.342\n",
      "Step 645: Loss=1.998, Acc=0.338\n",
      "Step 650: Loss=1.994, Acc=0.329\n",
      "Step 651: Loss=1.993, Acc=0.331\n",
      "Step 664: Loss=1.986, Acc=0.361\n",
      "Step 665: Loss=1.981, Acc=0.340\n",
      "Step 671: Loss=1.977, Acc=0.332\n",
      "Step 684: Loss=1.968, Acc=0.333\n",
      "Step 703: Loss=1.967, Acc=0.356\n",
      "Step 710: Loss=1.957, Acc=0.335\n",
      "Step 716: Loss=1.952, Acc=0.371\n",
      "Step 719: Loss=1.950, Acc=0.394\n",
      "Step 730: Loss=1.949, Acc=0.392\n",
      "Step 731: Loss=1.933, Acc=0.429\n",
      "Step 735: Loss=1.922, Acc=0.440\n",
      "Step 741: Loss=1.920, Acc=0.448\n",
      "Step 743: Loss=1.912, Acc=0.441\n",
      "Step 750: Loss=1.904, Acc=0.435\n",
      "Step 751: Loss=1.900, Acc=0.457\n",
      "Step 770: Loss=1.893, Acc=0.486\n",
      "Step 779: Loss=1.892, Acc=0.472\n",
      "Step 781: Loss=1.891, Acc=0.421\n",
      "Step 789: Loss=1.890, Acc=0.382\n",
      "Step 791: Loss=1.882, Acc=0.372\n",
      "Step 793: Loss=1.882, Acc=0.409\n",
      "Step 805: Loss=1.877, Acc=0.407\n",
      "Step 830: Loss=1.874, Acc=0.367\n",
      "Step 838: Loss=1.874, Acc=0.365\n",
      "Step 839: Loss=1.852, Acc=0.398\n",
      "Step 841: Loss=1.845, Acc=0.425\n",
      "Step 850: Loss=1.832, Acc=0.398\n",
      "Step 855: Loss=1.827, Acc=0.441\n",
      "Step 861: Loss=1.825, Acc=0.442\n",
      "Step 868: Loss=1.814, Acc=0.425\n",
      "Step 879: Loss=1.813, Acc=0.412\n",
      "Step 880: Loss=1.813, Acc=0.418\n",
      "Step 882: Loss=1.799, Acc=0.462\n",
      "Step 887: Loss=1.795, Acc=0.489\n",
      "Step 891: Loss=1.787, Acc=0.461\n",
      "Step 903: Loss=1.784, Acc=0.475\n",
      "Step 904: Loss=1.779, Acc=0.498\n",
      "Step 917: Loss=1.774, Acc=0.487\n",
      "Step 927: Loss=1.770, Acc=0.492\n",
      "Step 939: Loss=1.761, Acc=0.470\n",
      "Step 945: Loss=1.760, Acc=0.437\n",
      "Step 949: Loss=1.759, Acc=0.453\n",
      "Step 950: Loss=1.752, Acc=0.477\n",
      "Step 953: Loss=1.748, Acc=0.460\n",
      "Step 964: Loss=1.748, Acc=0.454\n",
      "Step 1010: Loss=1.745, Acc=0.481\n",
      "Step 1017: Loss=1.743, Acc=0.469\n",
      "Step 1020: Loss=1.743, Acc=0.440\n",
      "Step 1030: Loss=1.739, Acc=0.463\n",
      "Step 1034: Loss=1.736, Acc=0.464\n",
      "Step 1044: Loss=1.733, Acc=0.449\n",
      "Step 1057: Loss=1.727, Acc=0.482\n",
      "Step 1071: Loss=1.725, Acc=0.472\n",
      "Step 1076: Loss=1.713, Acc=0.451\n",
      "Step 1093: Loss=1.712, Acc=0.467\n",
      "Step 1095: Loss=1.711, Acc=0.443\n",
      "Step 1107: Loss=1.697, Acc=0.478\n",
      "Step 1110: Loss=1.695, Acc=0.484\n",
      "Step 1115: Loss=1.694, Acc=0.471\n",
      "Step 1128: Loss=1.687, Acc=0.465\n",
      "Step 1132: Loss=1.685, Acc=0.476\n",
      "Step 1140: Loss=1.677, Acc=0.482\n",
      "Step 1141: Loss=1.675, Acc=0.467\n",
      "Step 1147: Loss=1.672, Acc=0.468\n",
      "Step 1152: Loss=1.660, Acc=0.491\n",
      "Step 1168: Loss=1.656, Acc=0.486\n",
      "Step 1170: Loss=1.653, Acc=0.499\n",
      "Step 1197: Loss=1.652, Acc=0.508\n",
      "Step 1221: Loss=1.643, Acc=0.497\n",
      "Step 1225: Loss=1.624, Acc=0.508\n",
      "Step 1226: Loss=1.616, Acc=0.487\n",
      "Step 1240: Loss=1.605, Acc=0.518\n",
      "Step 1248: Loss=1.599, Acc=0.526\n",
      "Step 1252: Loss=1.595, Acc=0.511\n",
      "Step 1253: Loss=1.592, Acc=0.502\n",
      "Step 1261: Loss=1.590, Acc=0.518\n",
      "Step 1272: Loss=1.584, Acc=0.514\n",
      "Step 1287: Loss=1.584, Acc=0.465\n",
      "Step 1289: Loss=1.583, Acc=0.471\n",
      "Step 1291: Loss=1.579, Acc=0.496\n",
      "Step 1310: Loss=1.577, Acc=0.518\n",
      "Step 1340: Loss=1.577, Acc=0.516\n",
      "Step 1354: Loss=1.573, Acc=0.505\n",
      "Step 1359: Loss=1.558, Acc=0.538\n",
      "Step 1370: Loss=1.557, Acc=0.515\n",
      "Step 1389: Loss=1.553, Acc=0.531\n",
      "Step 1397: Loss=1.552, Acc=0.522\n",
      "Step 1404: Loss=1.549, Acc=0.508\n",
      "Step 1418: Loss=1.546, Acc=0.516\n",
      "Step 1420: Loss=1.546, Acc=0.494\n",
      "Step 1427: Loss=1.527, Acc=0.525\n",
      "Step 1444: Loss=1.520, Acc=0.537\n",
      "Step 1446: Loss=1.514, Acc=0.538\n",
      "Step 1449: Loss=1.509, Acc=0.552\n",
      "Step 1478: Loss=1.497, Acc=0.538\n",
      "Step 1495: Loss=1.490, Acc=0.566\n",
      "Step 1555: Loss=1.482, Acc=0.570\n",
      "Step 1569: Loss=1.480, Acc=0.546\n",
      "Step 1579: Loss=1.479, Acc=0.560\n",
      "Step 1590: Loss=1.464, Acc=0.546\n",
      "Step 1593: Loss=1.463, Acc=0.570\n",
      "Step 1594: Loss=1.462, Acc=0.574\n",
      "Step 1601: Loss=1.461, Acc=0.560\n",
      "Step 1611: Loss=1.452, Acc=0.558\n",
      "Step 1615: Loss=1.445, Acc=0.549\n",
      "Step 1616: Loss=1.441, Acc=0.538\n",
      "Step 1626: Loss=1.441, Acc=0.550\n",
      "Step 1629: Loss=1.441, Acc=0.538\n",
      "Step 1633: Loss=1.431, Acc=0.562\n",
      "Step 1634: Loss=1.420, Acc=0.579\n",
      "Step 1641: Loss=1.403, Acc=0.562\n",
      "Step 1658: Loss=1.391, Acc=0.595\n",
      "Step 1690: Loss=1.386, Acc=0.585\n",
      "Step 1694: Loss=1.386, Acc=0.588\n",
      "Step 1701: Loss=1.383, Acc=0.577\n",
      "Step 1707: Loss=1.378, Acc=0.595\n",
      "Step 1716: Loss=1.375, Acc=0.574\n",
      "Step 1729: Loss=1.374, Acc=0.568\n",
      "Step 1732: Loss=1.372, Acc=0.570\n",
      "Step 1739: Loss=1.357, Acc=0.602\n",
      "Step 1747: Loss=1.355, Acc=0.571\n",
      "Step 1781: Loss=1.349, Acc=0.582\n",
      "Step 1786: Loss=1.337, Acc=0.571\n",
      "Step 1804: Loss=1.334, Acc=0.568\n",
      "Step 1805: Loss=1.328, Acc=0.576\n",
      "Step 1822: Loss=1.327, Acc=0.571\n",
      "Step 1823: Loss=1.325, Acc=0.569\n",
      "Step 1826: Loss=1.320, Acc=0.573\n",
      "Step 1858: Loss=1.316, Acc=0.576\n",
      "Step 1864: Loss=1.315, Acc=0.575\n",
      "Step 1865: Loss=1.313, Acc=0.600\n",
      "Step 1868: Loss=1.306, Acc=0.587\n",
      "Step 1873: Loss=1.294, Acc=0.607\n",
      "Step 1885: Loss=1.289, Acc=0.604\n",
      "Step 1909: Loss=1.276, Acc=0.614\n",
      "Step 1950: Loss=1.267, Acc=0.615\n",
      "Step 1969: Loss=1.262, Acc=0.619\n",
      "Step 2026: Loss=1.257, Acc=0.624\n",
      "Step 2033: Loss=1.255, Acc=0.622\n",
      "Step 2041: Loss=1.255, Acc=0.609\n",
      "Step 2076: Loss=1.254, Acc=0.624\n",
      "Step 2083: Loss=1.251, Acc=0.606\n",
      "Step 2138: Loss=1.251, Acc=0.607\n",
      "Step 2157: Loss=1.246, Acc=0.600\n",
      "Step 2174: Loss=1.245, Acc=0.618\n",
      "Step 2188: Loss=1.245, Acc=0.611\n",
      "Step 2194: Loss=1.237, Acc=0.625\n",
      "Step 2422: Loss=1.234, Acc=0.634\n",
      "Step 2480: Loss=1.228, Acc=0.632\n",
      "Step 2482: Loss=1.228, Acc=0.630\n",
      "Step 2483: Loss=1.210, Acc=0.628\n",
      "Step 2536: Loss=1.210, Acc=0.628\n",
      "Step 2538: Loss=1.205, Acc=0.634\n",
      "Step 2539: Loss=1.205, Acc=0.632\n",
      "Step 2594: Loss=1.204, Acc=0.633\n",
      "Step 2619: Loss=1.191, Acc=0.636\n",
      "Step 2679: Loss=1.188, Acc=0.659\n",
      "Step 2700: Loss=1.185, Acc=0.632\n",
      "Step 2765: Loss=1.185, Acc=0.641\n",
      "Step 2864: Loss=1.182, Acc=0.617\n",
      "Step 2905: Loss=1.180, Acc=0.637\n",
      "Step 2932: Loss=1.173, Acc=0.643\n",
      "Step 2952: Loss=1.168, Acc=0.646\n",
      "Step 3054: Loss=1.167, Acc=0.629\n",
      "Step 3077: Loss=1.166, Acc=0.623\n",
      "Step 3080: Loss=1.156, Acc=0.642\n",
      "Step 3085: Loss=1.154, Acc=0.647\n",
      "Step 3096: Loss=1.153, Acc=0.649\n",
      "Step 3102: Loss=1.152, Acc=0.648\n",
      "Step 3139: Loss=1.135, Acc=0.657\n",
      "Step 3164: Loss=1.133, Acc=0.652\n",
      "Step 3184: Loss=1.128, Acc=0.647\n",
      "Step 3216: Loss=1.123, Acc=0.656\n",
      "Step 3241: Loss=1.122, Acc=0.651\n",
      "Step 3254: Loss=1.122, Acc=0.653\n",
      "Step 3259: Loss=1.121, Acc=0.652\n",
      "Step 3271: Loss=1.113, Acc=0.641\n",
      "Step 3273: Loss=1.110, Acc=0.658\n",
      "Step 3324: Loss=1.108, Acc=0.672\n",
      "Step 3333: Loss=1.106, Acc=0.673\n",
      "Step 3334: Loss=1.105, Acc=0.669\n",
      "Step 3366: Loss=1.099, Acc=0.666\n",
      "Step 3397: Loss=1.095, Acc=0.656\n",
      "Step 3429: Loss=1.095, Acc=0.675\n",
      "Step 3492: Loss=1.093, Acc=0.671\n",
      "Step 3591: Loss=1.089, Acc=0.666\n",
      "Step 3593: Loss=1.088, Acc=0.642\n",
      "Step 3595: Loss=1.087, Acc=0.648\n",
      "Step 3608: Loss=1.084, Acc=0.659\n",
      "Step 3620: Loss=1.080, Acc=0.674\n",
      "Step 3668: Loss=1.080, Acc=0.674\n",
      "Step 3717: Loss=1.067, Acc=0.683\n",
      "Step 3848: Loss=1.067, Acc=0.683\n",
      "Step 3907: Loss=1.067, Acc=0.680\n",
      "Step 3933: Loss=1.064, Acc=0.679\n",
      "Step 4165: Loss=1.061, Acc=0.686\n",
      "Step 4178: Loss=1.061, Acc=0.687\n",
      "Step 4235: Loss=1.061, Acc=0.671\n",
      "Step 4244: Loss=1.047, Acc=0.687\n",
      "Step 4248: Loss=1.044, Acc=0.690\n",
      "Step 4351: Loss=1.042, Acc=0.686\n",
      "Step 4357: Loss=1.034, Acc=0.692\n",
      "Step 4406: Loss=1.029, Acc=0.683\n",
      "Step 4422: Loss=1.002, Acc=0.702\n",
      "Step 4428: Loss=0.993, Acc=0.682\n",
      "Step 4434: Loss=0.992, Acc=0.689\n",
      "Step 4447: Loss=0.990, Acc=0.692\n",
      "Step 4469: Loss=0.987, Acc=0.674\n",
      "Step 4508: Loss=0.986, Acc=0.686\n",
      "Step 4522: Loss=0.982, Acc=0.697\n",
      "Step 4545: Loss=0.966, Acc=0.698\n",
      "Step 4551: Loss=0.955, Acc=0.712\n",
      "Step 4621: Loss=0.954, Acc=0.703\n",
      "Step 4719: Loss=0.950, Acc=0.697\n",
      "Step 4844: Loss=0.947, Acc=0.703\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "print(\"Starting training...\")\n",
    "best_loss = 999\n",
    "for i in range(5000):\n",
    "    # Random weight updates for all layers\n",
    "    model.w1 += 0.01 * np.random.randn(*model.w1.shape)\n",
    "    model.b1 += 0.01 * np.random.randn(*model.b1.shape)\n",
    "    model.w2 += 0.01 * np.random.randn(*model.w2.shape)\n",
    "    model.b2 += 0.01 * np.random.randn(*model.b2.shape)\n",
    "    model.w3 += 0.01 * np.random.randn(*model.w3.shape)\n",
    "    model.b3 += 0.01 * np.random.randn(*model.b3.shape)\n",
    "    \n",
    "    pred = model.forward(X)\n",
    "    loss = model.loss(pred, y)\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        model.save(\"Turki2236938Model.npz\")\n",
    "        acc = np.mean(np.argmax(pred, axis=1) == y)\n",
    "        print(f\"Step {i}: Loss={loss:.3f}, Acc={acc:.3f}\")\n",
    "    else:\n",
    "        model.load(\"Turki2236938Model.npz\")\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac193741",
   "metadata": {},
   "source": [
    "# **Loading Result Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c10f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(\"Turki2236938Model.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccb841",
   "metadata": {},
   "source": [
    "# **Test on shared test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3239585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shared test set...\n",
      "Shared Test Accuracy: 0.700\n"
     ]
    }
   ],
   "source": [
    "test_path = \"Ai2_Dataset/SharedTesting\"\n",
    "if os.path.exists(test_path):\n",
    "    print(\"Loading shared test set...\")\n",
    "    test_X, test_y = load_data(test_path)\n",
    "    test_pred = model.forward(test_X)\n",
    "    test_acc = np.mean(np.argmax(test_pred, axis=1) == test_y)\n",
    "    print(f\"Shared Test Accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfccc61d",
   "metadata": {},
   "source": [
    "# **Print every prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8cba0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Predicted=3, Actual=0\n",
      "Sample 1: Predicted=0, Actual=0\n",
      "Sample 2: Predicted=0, Actual=0\n",
      "Sample 3: Predicted=0, Actual=0\n",
      "Sample 4: Predicted=9, Actual=0\n",
      "Sample 5: Predicted=3, Actual=0\n",
      "Sample 6: Predicted=0, Actual=0\n",
      "Sample 7: Predicted=0, Actual=0\n",
      "Sample 8: Predicted=0, Actual=0\n",
      "Sample 9: Predicted=0, Actual=0\n",
      "Sample 10: Predicted=1, Actual=1\n",
      "Sample 11: Predicted=1, Actual=1\n",
      "Sample 12: Predicted=1, Actual=1\n",
      "Sample 13: Predicted=1, Actual=1\n",
      "Sample 14: Predicted=4, Actual=1\n",
      "Sample 15: Predicted=1, Actual=1\n",
      "Sample 16: Predicted=1, Actual=1\n",
      "Sample 17: Predicted=1, Actual=1\n",
      "Sample 18: Predicted=4, Actual=1\n",
      "Sample 19: Predicted=1, Actual=1\n",
      "Sample 20: Predicted=2, Actual=2\n",
      "Sample 21: Predicted=3, Actual=2\n",
      "Sample 22: Predicted=2, Actual=2\n",
      "Sample 23: Predicted=2, Actual=2\n",
      "Sample 24: Predicted=2, Actual=2\n",
      "Sample 25: Predicted=9, Actual=2\n",
      "Sample 26: Predicted=2, Actual=2\n",
      "Sample 27: Predicted=2, Actual=2\n",
      "Sample 28: Predicted=2, Actual=2\n",
      "Sample 29: Predicted=3, Actual=2\n",
      "Sample 30: Predicted=3, Actual=3\n",
      "Sample 31: Predicted=9, Actual=3\n",
      "Sample 32: Predicted=7, Actual=3\n",
      "Sample 33: Predicted=2, Actual=3\n",
      "Sample 34: Predicted=3, Actual=3\n",
      "Sample 35: Predicted=3, Actual=3\n",
      "Sample 36: Predicted=3, Actual=3\n",
      "Sample 37: Predicted=3, Actual=3\n",
      "Sample 38: Predicted=2, Actual=3\n",
      "Sample 39: Predicted=3, Actual=3\n",
      "Sample 40: Predicted=4, Actual=4\n",
      "Sample 41: Predicted=4, Actual=4\n",
      "Sample 42: Predicted=4, Actual=4\n",
      "Sample 43: Predicted=3, Actual=4\n",
      "Sample 44: Predicted=4, Actual=4\n",
      "Sample 45: Predicted=4, Actual=4\n",
      "Sample 46: Predicted=4, Actual=4\n",
      "Sample 47: Predicted=4, Actual=4\n",
      "Sample 48: Predicted=4, Actual=4\n",
      "Sample 49: Predicted=4, Actual=4\n",
      "Sample 50: Predicted=5, Actual=5\n",
      "Sample 51: Predicted=5, Actual=5\n",
      "Sample 52: Predicted=5, Actual=5\n",
      "Sample 53: Predicted=5, Actual=5\n",
      "Sample 54: Predicted=1, Actual=5\n",
      "Sample 55: Predicted=5, Actual=5\n",
      "Sample 56: Predicted=1, Actual=5\n",
      "Sample 57: Predicted=5, Actual=5\n",
      "Sample 58: Predicted=5, Actual=5\n",
      "Sample 59: Predicted=8, Actual=5\n",
      "Sample 60: Predicted=6, Actual=6\n",
      "Sample 61: Predicted=9, Actual=6\n",
      "Sample 62: Predicted=6, Actual=6\n",
      "Sample 63: Predicted=6, Actual=6\n",
      "Sample 64: Predicted=6, Actual=6\n",
      "Sample 65: Predicted=6, Actual=6\n",
      "Sample 66: Predicted=6, Actual=6\n",
      "Sample 67: Predicted=6, Actual=6\n",
      "Sample 68: Predicted=9, Actual=6\n",
      "Sample 69: Predicted=6, Actual=6\n",
      "Sample 70: Predicted=6, Actual=7\n",
      "Sample 71: Predicted=7, Actual=7\n",
      "Sample 72: Predicted=3, Actual=7\n",
      "Sample 73: Predicted=7, Actual=7\n",
      "Sample 74: Predicted=7, Actual=7\n",
      "Sample 75: Predicted=7, Actual=7\n",
      "Sample 76: Predicted=7, Actual=7\n",
      "Sample 77: Predicted=8, Actual=7\n",
      "Sample 78: Predicted=3, Actual=7\n",
      "Sample 79: Predicted=3, Actual=7\n",
      "Sample 80: Predicted=8, Actual=8\n",
      "Sample 81: Predicted=9, Actual=8\n",
      "Sample 82: Predicted=8, Actual=8\n",
      "Sample 83: Predicted=8, Actual=8\n",
      "Sample 84: Predicted=6, Actual=8\n",
      "Sample 85: Predicted=8, Actual=8\n",
      "Sample 86: Predicted=8, Actual=8\n",
      "Sample 87: Predicted=8, Actual=8\n",
      "Sample 88: Predicted=8, Actual=8\n",
      "Sample 89: Predicted=8, Actual=8\n",
      "Sample 90: Predicted=6, Actual=9\n",
      "Sample 91: Predicted=9, Actual=9\n",
      "Sample 92: Predicted=9, Actual=9\n",
      "Sample 93: Predicted=9, Actual=9\n",
      "Sample 94: Predicted=9, Actual=9\n",
      "Sample 95: Predicted=6, Actual=9\n",
      "Sample 96: Predicted=6, Actual=9\n",
      "Sample 97: Predicted=9, Actual=9\n",
      "Sample 98: Predicted=6, Actual=9\n",
      "Sample 99: Predicted=6, Actual=9\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for i, pred in enumerate(test_pred):\n",
    "    predicted_class = np.argmax(pred)\n",
    "    actual_class = test_y[i]\n",
    "    print(f\"Sample {i}: Predicted={predicted_class}, Actual={actual_class}\")\n",
    "else:\n",
    "    print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
