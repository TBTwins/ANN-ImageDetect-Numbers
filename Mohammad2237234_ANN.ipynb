{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa4a95b",
   "metadata": {},
   "source": [
    "#### Name: Mohammad Ahmad Abutalib\n",
    "#### ID: 2237234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3601b",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4920fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fd458",
   "metadata": {},
   "source": [
    "# Set My Own Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b1dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducible results\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd0213",
   "metadata": {},
   "source": [
    "# Creating ANN Class with Needed Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e548809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    def __init__(self, input_size):\n",
    "        # First hidden layer\n",
    "        self.w1 = 0.00885123 * np.random.randn(input_size, 32)\n",
    "        self.b1 = np.zeros((1, 32))\n",
    "        # Second hidden layer\n",
    "        self.w2 = 0.00885123 * np.random.randn(32, 16)\n",
    "        self.b2 = np.zeros((1, 16))\n",
    "        # Output layer\n",
    "        self.w3 = 0.00885123 * np.random.randn(16, 10)\n",
    "        self.b3 = np.zeros((1, 10))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # First hidden layer\n",
    "        z1 = np.dot(X, self.w1) + self.b1\n",
    "        a1 = np.maximum(0, z1)  # ReLU\n",
    "        # Second hidden layer\n",
    "        z2 = np.dot(a1, self.w2) + self.b2\n",
    "        a2 = np.maximum(0, z2)  # ReLU\n",
    "        # Output layer\n",
    "        z3 = np.dot(a2, self.w3) + self.b3\n",
    "        exp_z = np.exp(z3 - np.max(z3, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)  # Softmax\n",
    "    \n",
    "    def loss(self, pred, y):\n",
    "        return -np.mean(np.log(np.clip(pred[range(len(pred)), y], 1e-7, 1)))\n",
    "    \n",
    "    def save(self, name):\n",
    "        np.savez(name, w1=self.w1, b1=self.b1, w2=self.w2, b2=self.b2, w3=self.w3, b3=self.b3)\n",
    "    \n",
    "    def load(self, name):\n",
    "        data = np.load(name)\n",
    "        self.w1, self.b1 = data['w1'], data['b1']\n",
    "        self.w2, self.b2 = data['w2'], data['b2']\n",
    "        self.w3, self.b3 = data['w3'], data['b3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5fadd",
   "metadata": {},
   "source": [
    "# Creating Data Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c830824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    X, y = [], []\n",
    "    for digit in range(10):\n",
    "        folder = os.path.join(path, str(digit))\n",
    "        if os.path.exists(folder):\n",
    "            for file in os.listdir(folder):\n",
    "                if file.endswith(('.png')):\n",
    "                    img = Image.open(os.path.join(folder, file))\n",
    "                    arr = np.array(img, dtype=np.float64)\n",
    "                    flat_arr = arr.flatten()\n",
    "                    if flat_arr.max() > 1: flat_arr /= 255.0\n",
    "                    X.append(flat_arr)\n",
    "                    y.append(digit)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abaae2",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be552424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1980 images\n",
      "Image shape: (784,)\n",
      "Classes: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_data(\"Ai2_Dataset/Mohammad\")\n",
    "print(f\"Loaded {len(X)} images\")\n",
    "print(f\"Image shape: {X[0].shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6351b",
   "metadata": {},
   "source": [
    "# **Creating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6b54d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created with input size: 784\n",
      "Architecture: 784 -> 32 -> 16 -> 10\n"
     ]
    }
   ],
   "source": [
    "#Create model\n",
    "model = ANN(X.shape[1])\n",
    "print(f\"Model created with input size: {X.shape[1]}\")\n",
    "print(f\"Architecture: {X.shape[1]} -> 32 -> 16 -> 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8df796d",
   "metadata": {},
   "source": [
    "# **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55486c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Step 0: Loss=2.303, Acc=0.100\n",
      "Step 4: Loss=2.303, Acc=0.100\n",
      "Step 5: Loss=2.303, Acc=0.100\n",
      "Step 8: Loss=2.303, Acc=0.100\n",
      "Step 10: Loss=2.303, Acc=0.100\n",
      "Step 14: Loss=2.303, Acc=0.100\n",
      "Step 16: Loss=2.302, Acc=0.100\n",
      "Step 17: Loss=2.302, Acc=0.101\n",
      "Step 18: Loss=2.302, Acc=0.122\n",
      "Step 19: Loss=2.302, Acc=0.100\n",
      "Step 22: Loss=2.302, Acc=0.103\n",
      "Step 24: Loss=2.302, Acc=0.100\n",
      "Step 25: Loss=2.302, Acc=0.100\n",
      "Step 27: Loss=2.301, Acc=0.100\n",
      "Step 28: Loss=2.301, Acc=0.100\n",
      "Step 30: Loss=2.301, Acc=0.100\n",
      "Step 31: Loss=2.301, Acc=0.103\n",
      "Step 32: Loss=2.300, Acc=0.100\n",
      "Step 36: Loss=2.300, Acc=0.100\n",
      "Step 37: Loss=2.300, Acc=0.105\n",
      "Step 46: Loss=2.299, Acc=0.103\n",
      "Step 52: Loss=2.299, Acc=0.101\n",
      "Step 53: Loss=2.299, Acc=0.104\n",
      "Step 61: Loss=2.299, Acc=0.105\n",
      "Step 65: Loss=2.298, Acc=0.123\n",
      "Step 69: Loss=2.298, Acc=0.128\n",
      "Step 78: Loss=2.298, Acc=0.109\n",
      "Step 83: Loss=2.297, Acc=0.128\n",
      "Step 84: Loss=2.297, Acc=0.100\n",
      "Step 87: Loss=2.297, Acc=0.101\n",
      "Step 88: Loss=2.296, Acc=0.106\n",
      "Step 90: Loss=2.295, Acc=0.127\n",
      "Step 92: Loss=2.294, Acc=0.163\n",
      "Step 98: Loss=2.294, Acc=0.163\n",
      "Step 104: Loss=2.294, Acc=0.221\n",
      "Step 108: Loss=2.292, Acc=0.125\n",
      "Step 112: Loss=2.291, Acc=0.164\n",
      "Step 118: Loss=2.290, Acc=0.195\n",
      "Step 124: Loss=2.290, Acc=0.144\n",
      "Step 125: Loss=2.289, Acc=0.128\n",
      "Step 126: Loss=2.288, Acc=0.108\n",
      "Step 130: Loss=2.287, Acc=0.144\n",
      "Step 132: Loss=2.286, Acc=0.124\n",
      "Step 139: Loss=2.286, Acc=0.123\n",
      "Step 144: Loss=2.285, Acc=0.108\n",
      "Step 149: Loss=2.285, Acc=0.103\n",
      "Step 152: Loss=2.285, Acc=0.157\n",
      "Step 154: Loss=2.284, Acc=0.156\n",
      "Step 156: Loss=2.283, Acc=0.104\n",
      "Step 158: Loss=2.283, Acc=0.174\n",
      "Step 163: Loss=2.282, Acc=0.104\n",
      "Step 164: Loss=2.281, Acc=0.105\n",
      "Step 165: Loss=2.280, Acc=0.135\n",
      "Step 168: Loss=2.280, Acc=0.154\n",
      "Step 173: Loss=2.279, Acc=0.101\n",
      "Step 178: Loss=2.277, Acc=0.128\n",
      "Step 185: Loss=2.275, Acc=0.132\n",
      "Step 191: Loss=2.275, Acc=0.112\n",
      "Step 193: Loss=2.274, Acc=0.130\n",
      "Step 195: Loss=2.274, Acc=0.152\n",
      "Step 198: Loss=2.272, Acc=0.176\n",
      "Step 200: Loss=2.270, Acc=0.169\n",
      "Step 201: Loss=2.269, Acc=0.185\n",
      "Step 206: Loss=2.269, Acc=0.174\n",
      "Step 207: Loss=2.268, Acc=0.164\n",
      "Step 211: Loss=2.266, Acc=0.204\n",
      "Step 213: Loss=2.265, Acc=0.177\n",
      "Step 216: Loss=2.264, Acc=0.138\n",
      "Step 218: Loss=2.263, Acc=0.163\n",
      "Step 220: Loss=2.262, Acc=0.164\n",
      "Step 222: Loss=2.262, Acc=0.154\n",
      "Step 227: Loss=2.259, Acc=0.164\n",
      "Step 229: Loss=2.258, Acc=0.147\n",
      "Step 230: Loss=2.254, Acc=0.119\n",
      "Step 232: Loss=2.254, Acc=0.125\n",
      "Step 233: Loss=2.253, Acc=0.137\n",
      "Step 234: Loss=2.250, Acc=0.140\n",
      "Step 238: Loss=2.248, Acc=0.130\n",
      "Step 244: Loss=2.246, Acc=0.124\n",
      "Step 247: Loss=2.245, Acc=0.172\n",
      "Step 249: Loss=2.245, Acc=0.141\n",
      "Step 252: Loss=2.245, Acc=0.160\n",
      "Step 253: Loss=2.244, Acc=0.120\n",
      "Step 254: Loss=2.240, Acc=0.209\n",
      "Step 257: Loss=2.237, Acc=0.203\n",
      "Step 259: Loss=2.236, Acc=0.219\n",
      "Step 261: Loss=2.235, Acc=0.161\n",
      "Step 270: Loss=2.234, Acc=0.185\n",
      "Step 272: Loss=2.231, Acc=0.171\n",
      "Step 274: Loss=2.229, Acc=0.184\n",
      "Step 286: Loss=2.228, Acc=0.169\n",
      "Step 289: Loss=2.227, Acc=0.216\n",
      "Step 302: Loss=2.225, Acc=0.248\n",
      "Step 305: Loss=2.224, Acc=0.238\n",
      "Step 319: Loss=2.222, Acc=0.226\n",
      "Step 323: Loss=2.219, Acc=0.213\n",
      "Step 332: Loss=2.216, Acc=0.217\n",
      "Step 338: Loss=2.215, Acc=0.195\n",
      "Step 342: Loss=2.212, Acc=0.236\n",
      "Step 345: Loss=2.211, Acc=0.254\n",
      "Step 348: Loss=2.209, Acc=0.282\n",
      "Step 350: Loss=2.206, Acc=0.214\n",
      "Step 360: Loss=2.205, Acc=0.234\n",
      "Step 362: Loss=2.202, Acc=0.228\n",
      "Step 364: Loss=2.198, Acc=0.258\n",
      "Step 369: Loss=2.197, Acc=0.286\n",
      "Step 370: Loss=2.197, Acc=0.281\n",
      "Step 386: Loss=2.195, Acc=0.249\n",
      "Step 390: Loss=2.194, Acc=0.268\n",
      "Step 402: Loss=2.189, Acc=0.256\n",
      "Step 404: Loss=2.184, Acc=0.246\n",
      "Step 406: Loss=2.184, Acc=0.244\n",
      "Step 426: Loss=2.176, Acc=0.260\n",
      "Step 428: Loss=2.172, Acc=0.257\n",
      "Step 448: Loss=2.171, Acc=0.290\n",
      "Step 451: Loss=2.165, Acc=0.276\n",
      "Step 454: Loss=2.163, Acc=0.259\n",
      "Step 470: Loss=2.161, Acc=0.283\n",
      "Step 472: Loss=2.160, Acc=0.226\n",
      "Step 473: Loss=2.157, Acc=0.243\n",
      "Step 480: Loss=2.153, Acc=0.253\n",
      "Step 482: Loss=2.151, Acc=0.278\n",
      "Step 487: Loss=2.147, Acc=0.300\n",
      "Step 490: Loss=2.144, Acc=0.297\n",
      "Step 492: Loss=2.139, Acc=0.286\n",
      "Step 494: Loss=2.137, Acc=0.316\n",
      "Step 496: Loss=2.136, Acc=0.256\n",
      "Step 497: Loss=2.136, Acc=0.246\n",
      "Step 498: Loss=2.133, Acc=0.253\n",
      "Step 500: Loss=2.132, Acc=0.246\n",
      "Step 503: Loss=2.129, Acc=0.242\n",
      "Step 504: Loss=2.129, Acc=0.257\n",
      "Step 505: Loss=2.127, Acc=0.273\n",
      "Step 511: Loss=2.118, Acc=0.284\n",
      "Step 512: Loss=2.116, Acc=0.265\n",
      "Step 514: Loss=2.114, Acc=0.280\n",
      "Step 519: Loss=2.111, Acc=0.270\n",
      "Step 521: Loss=2.108, Acc=0.296\n",
      "Step 527: Loss=2.108, Acc=0.300\n",
      "Step 529: Loss=2.100, Acc=0.330\n",
      "Step 535: Loss=2.097, Acc=0.332\n",
      "Step 540: Loss=2.092, Acc=0.327\n",
      "Step 543: Loss=2.089, Acc=0.339\n",
      "Step 545: Loss=2.083, Acc=0.393\n",
      "Step 547: Loss=2.068, Acc=0.389\n",
      "Step 551: Loss=2.064, Acc=0.406\n",
      "Step 553: Loss=2.063, Acc=0.390\n",
      "Step 554: Loss=2.063, Acc=0.354\n",
      "Step 557: Loss=2.053, Acc=0.361\n",
      "Step 558: Loss=2.048, Acc=0.346\n",
      "Step 560: Loss=2.047, Acc=0.331\n",
      "Step 561: Loss=2.046, Acc=0.322\n",
      "Step 562: Loss=2.045, Acc=0.304\n",
      "Step 563: Loss=2.042, Acc=0.291\n",
      "Step 565: Loss=2.031, Acc=0.303\n",
      "Step 566: Loss=2.031, Acc=0.291\n",
      "Step 569: Loss=2.013, Acc=0.337\n",
      "Step 571: Loss=2.009, Acc=0.372\n",
      "Step 574: Loss=2.007, Acc=0.380\n",
      "Step 578: Loss=2.002, Acc=0.358\n",
      "Step 586: Loss=1.999, Acc=0.341\n",
      "Step 587: Loss=1.997, Acc=0.293\n",
      "Step 595: Loss=1.979, Acc=0.319\n",
      "Step 597: Loss=1.978, Acc=0.365\n",
      "Step 600: Loss=1.967, Acc=0.370\n",
      "Step 607: Loss=1.965, Acc=0.390\n",
      "Step 610: Loss=1.962, Acc=0.372\n",
      "Step 613: Loss=1.956, Acc=0.409\n",
      "Step 614: Loss=1.949, Acc=0.387\n",
      "Step 623: Loss=1.948, Acc=0.407\n",
      "Step 624: Loss=1.943, Acc=0.463\n",
      "Step 633: Loss=1.942, Acc=0.419\n",
      "Step 640: Loss=1.938, Acc=0.458\n",
      "Step 663: Loss=1.934, Acc=0.436\n",
      "Step 683: Loss=1.933, Acc=0.428\n",
      "Step 686: Loss=1.931, Acc=0.422\n",
      "Step 696: Loss=1.930, Acc=0.383\n",
      "Step 702: Loss=1.926, Acc=0.382\n",
      "Step 722: Loss=1.923, Acc=0.396\n",
      "Step 724: Loss=1.922, Acc=0.382\n",
      "Step 754: Loss=1.920, Acc=0.366\n",
      "Step 764: Loss=1.915, Acc=0.380\n",
      "Step 769: Loss=1.913, Acc=0.370\n",
      "Step 784: Loss=1.904, Acc=0.372\n",
      "Step 785: Loss=1.893, Acc=0.404\n",
      "Step 787: Loss=1.892, Acc=0.389\n",
      "Step 789: Loss=1.885, Acc=0.390\n",
      "Step 795: Loss=1.883, Acc=0.405\n",
      "Step 803: Loss=1.882, Acc=0.428\n",
      "Step 806: Loss=1.881, Acc=0.452\n",
      "Step 807: Loss=1.876, Acc=0.446\n",
      "Step 813: Loss=1.873, Acc=0.390\n",
      "Step 817: Loss=1.873, Acc=0.398\n",
      "Step 822: Loss=1.864, Acc=0.403\n",
      "Step 829: Loss=1.860, Acc=0.396\n",
      "Step 834: Loss=1.859, Acc=0.394\n",
      "Step 836: Loss=1.858, Acc=0.401\n",
      "Step 839: Loss=1.856, Acc=0.438\n",
      "Step 845: Loss=1.850, Acc=0.454\n",
      "Step 850: Loss=1.847, Acc=0.430\n",
      "Step 859: Loss=1.846, Acc=0.396\n",
      "Step 861: Loss=1.844, Acc=0.414\n",
      "Step 868: Loss=1.832, Acc=0.451\n",
      "Step 869: Loss=1.825, Acc=0.431\n",
      "Step 888: Loss=1.822, Acc=0.437\n",
      "Step 889: Loss=1.817, Acc=0.445\n",
      "Step 897: Loss=1.817, Acc=0.454\n",
      "Step 899: Loss=1.808, Acc=0.516\n",
      "Step 908: Loss=1.803, Acc=0.497\n",
      "Step 910: Loss=1.791, Acc=0.455\n",
      "Step 915: Loss=1.788, Acc=0.432\n",
      "Step 924: Loss=1.784, Acc=0.405\n",
      "Step 945: Loss=1.783, Acc=0.401\n",
      "Step 961: Loss=1.780, Acc=0.425\n",
      "Step 970: Loss=1.777, Acc=0.482\n",
      "Step 972: Loss=1.777, Acc=0.451\n",
      "Step 981: Loss=1.776, Acc=0.494\n",
      "Step 985: Loss=1.769, Acc=0.479\n",
      "Step 998: Loss=1.769, Acc=0.525\n",
      "Step 1013: Loss=1.767, Acc=0.529\n",
      "Step 1021: Loss=1.761, Acc=0.469\n",
      "Step 1022: Loss=1.759, Acc=0.470\n",
      "Step 1032: Loss=1.756, Acc=0.470\n",
      "Step 1038: Loss=1.750, Acc=0.483\n",
      "Step 1042: Loss=1.750, Acc=0.508\n",
      "Step 1043: Loss=1.744, Acc=0.480\n",
      "Step 1047: Loss=1.743, Acc=0.466\n",
      "Step 1058: Loss=1.736, Acc=0.431\n",
      "Step 1061: Loss=1.735, Acc=0.392\n",
      "Step 1065: Loss=1.731, Acc=0.408\n",
      "Step 1070: Loss=1.725, Acc=0.395\n",
      "Step 1071: Loss=1.722, Acc=0.443\n",
      "Step 1084: Loss=1.708, Acc=0.458\n",
      "Step 1086: Loss=1.702, Acc=0.499\n",
      "Step 1102: Loss=1.702, Acc=0.528\n",
      "Step 1104: Loss=1.694, Acc=0.517\n",
      "Step 1105: Loss=1.684, Acc=0.529\n",
      "Step 1106: Loss=1.676, Acc=0.528\n",
      "Step 1110: Loss=1.676, Acc=0.521\n",
      "Step 1119: Loss=1.666, Acc=0.525\n",
      "Step 1122: Loss=1.662, Acc=0.532\n",
      "Step 1129: Loss=1.658, Acc=0.531\n",
      "Step 1130: Loss=1.656, Acc=0.511\n",
      "Step 1134: Loss=1.649, Acc=0.536\n",
      "Step 1143: Loss=1.649, Acc=0.528\n",
      "Step 1153: Loss=1.647, Acc=0.511\n",
      "Step 1161: Loss=1.647, Acc=0.506\n",
      "Step 1170: Loss=1.645, Acc=0.525\n",
      "Step 1178: Loss=1.636, Acc=0.537\n",
      "Step 1182: Loss=1.630, Acc=0.555\n",
      "Step 1207: Loss=1.630, Acc=0.567\n",
      "Step 1213: Loss=1.624, Acc=0.566\n",
      "Step 1221: Loss=1.618, Acc=0.539\n",
      "Step 1225: Loss=1.615, Acc=0.536\n",
      "Step 1229: Loss=1.608, Acc=0.562\n",
      "Step 1240: Loss=1.603, Acc=0.537\n",
      "Step 1251: Loss=1.601, Acc=0.567\n",
      "Step 1262: Loss=1.597, Acc=0.571\n",
      "Step 1268: Loss=1.587, Acc=0.563\n",
      "Step 1270: Loss=1.586, Acc=0.593\n",
      "Step 1277: Loss=1.582, Acc=0.574\n",
      "Step 1282: Loss=1.577, Acc=0.564\n",
      "Step 1286: Loss=1.572, Acc=0.540\n",
      "Step 1290: Loss=1.569, Acc=0.542\n",
      "Step 1293: Loss=1.564, Acc=0.574\n",
      "Step 1313: Loss=1.555, Acc=0.633\n",
      "Step 1318: Loss=1.552, Acc=0.628\n",
      "Step 1321: Loss=1.541, Acc=0.634\n",
      "Step 1323: Loss=1.533, Acc=0.632\n",
      "Step 1328: Loss=1.533, Acc=0.616\n",
      "Step 1333: Loss=1.531, Acc=0.579\n",
      "Step 1349: Loss=1.530, Acc=0.546\n",
      "Step 1357: Loss=1.524, Acc=0.564\n",
      "Step 1363: Loss=1.515, Acc=0.554\n",
      "Step 1377: Loss=1.510, Acc=0.572\n",
      "Step 1388: Loss=1.505, Acc=0.623\n",
      "Step 1407: Loss=1.493, Acc=0.613\n",
      "Step 1412: Loss=1.492, Acc=0.597\n",
      "Step 1413: Loss=1.486, Acc=0.588\n",
      "Step 1414: Loss=1.473, Acc=0.582\n",
      "Step 1473: Loss=1.464, Acc=0.613\n",
      "Step 1475: Loss=1.462, Acc=0.628\n",
      "Step 1483: Loss=1.456, Acc=0.606\n",
      "Step 1489: Loss=1.454, Acc=0.575\n",
      "Step 1494: Loss=1.447, Acc=0.567\n",
      "Step 1496: Loss=1.443, Acc=0.579\n",
      "Step 1500: Loss=1.440, Acc=0.626\n",
      "Step 1514: Loss=1.431, Acc=0.621\n",
      "Step 1518: Loss=1.430, Acc=0.614\n",
      "Step 1523: Loss=1.424, Acc=0.650\n",
      "Step 1533: Loss=1.419, Acc=0.613\n",
      "Step 1540: Loss=1.413, Acc=0.627\n",
      "Step 1545: Loss=1.396, Acc=0.639\n",
      "Step 1552: Loss=1.395, Acc=0.618\n",
      "Step 1575: Loss=1.393, Acc=0.612\n",
      "Step 1582: Loss=1.389, Acc=0.676\n",
      "Step 1584: Loss=1.388, Acc=0.675\n",
      "Step 1589: Loss=1.384, Acc=0.661\n",
      "Step 1606: Loss=1.382, Acc=0.645\n",
      "Step 1621: Loss=1.376, Acc=0.626\n",
      "Step 1629: Loss=1.369, Acc=0.618\n",
      "Step 1652: Loss=1.365, Acc=0.612\n",
      "Step 1658: Loss=1.350, Acc=0.648\n",
      "Step 1661: Loss=1.348, Acc=0.666\n",
      "Step 1668: Loss=1.335, Acc=0.659\n",
      "Step 1675: Loss=1.335, Acc=0.683\n",
      "Step 1689: Loss=1.320, Acc=0.668\n",
      "Step 1728: Loss=1.316, Acc=0.681\n",
      "Step 1735: Loss=1.310, Acc=0.681\n",
      "Step 1738: Loss=1.302, Acc=0.669\n",
      "Step 1866: Loss=1.299, Acc=0.678\n",
      "Step 1872: Loss=1.293, Acc=0.659\n",
      "Step 1879: Loss=1.288, Acc=0.667\n",
      "Step 1882: Loss=1.280, Acc=0.671\n",
      "Step 1885: Loss=1.273, Acc=0.683\n",
      "Step 1928: Loss=1.269, Acc=0.701\n",
      "Step 1931: Loss=1.268, Acc=0.712\n",
      "Step 1945: Loss=1.268, Acc=0.699\n",
      "Step 1947: Loss=1.257, Acc=0.709\n",
      "Step 1954: Loss=1.250, Acc=0.704\n",
      "Step 1983: Loss=1.232, Acc=0.713\n",
      "Step 2007: Loss=1.231, Acc=0.685\n",
      "Step 2015: Loss=1.223, Acc=0.707\n",
      "Step 2019: Loss=1.216, Acc=0.710\n",
      "Step 2063: Loss=1.215, Acc=0.672\n",
      "Step 2067: Loss=1.210, Acc=0.682\n",
      "Step 2072: Loss=1.208, Acc=0.684\n",
      "Step 2094: Loss=1.197, Acc=0.698\n",
      "Step 2101: Loss=1.190, Acc=0.713\n",
      "Step 2118: Loss=1.181, Acc=0.709\n",
      "Step 2126: Loss=1.181, Acc=0.689\n",
      "Step 2130: Loss=1.174, Acc=0.710\n",
      "Step 2133: Loss=1.173, Acc=0.705\n",
      "Step 2161: Loss=1.164, Acc=0.686\n",
      "Step 2189: Loss=1.163, Acc=0.671\n",
      "Step 2199: Loss=1.151, Acc=0.676\n",
      "Step 2223: Loss=1.142, Acc=0.715\n",
      "Step 2225: Loss=1.141, Acc=0.713\n",
      "Step 2241: Loss=1.124, Acc=0.721\n",
      "Step 2265: Loss=1.122, Acc=0.709\n",
      "Step 2270: Loss=1.102, Acc=0.739\n",
      "Step 2274: Loss=1.101, Acc=0.727\n",
      "Step 2280: Loss=1.092, Acc=0.731\n",
      "Step 2307: Loss=1.085, Acc=0.753\n",
      "Step 2310: Loss=1.079, Acc=0.734\n",
      "Step 2359: Loss=1.069, Acc=0.741\n",
      "Step 2366: Loss=1.064, Acc=0.735\n",
      "Step 2401: Loss=1.064, Acc=0.743\n",
      "Step 2407: Loss=1.060, Acc=0.745\n",
      "Step 2453: Loss=1.052, Acc=0.760\n",
      "Step 2472: Loss=1.051, Acc=0.769\n",
      "Step 2482: Loss=1.050, Acc=0.752\n",
      "Step 2484: Loss=1.046, Acc=0.747\n",
      "Step 2527: Loss=1.042, Acc=0.734\n",
      "Step 2542: Loss=1.039, Acc=0.732\n",
      "Step 2587: Loss=1.038, Acc=0.732\n",
      "Step 2619: Loss=1.037, Acc=0.728\n",
      "Step 2654: Loss=1.026, Acc=0.731\n",
      "Step 2711: Loss=1.026, Acc=0.743\n",
      "Step 2721: Loss=1.022, Acc=0.734\n",
      "Step 2727: Loss=1.013, Acc=0.746\n",
      "Step 2760: Loss=1.009, Acc=0.747\n",
      "Step 2772: Loss=1.005, Acc=0.744\n",
      "Step 2795: Loss=1.001, Acc=0.720\n",
      "Step 2796: Loss=0.998, Acc=0.728\n",
      "Step 2797: Loss=0.997, Acc=0.749\n",
      "Step 2805: Loss=0.995, Acc=0.727\n",
      "Step 2817: Loss=0.987, Acc=0.738\n",
      "Step 2857: Loss=0.987, Acc=0.722\n",
      "Step 2860: Loss=0.981, Acc=0.729\n",
      "Step 2871: Loss=0.980, Acc=0.739\n",
      "Step 2883: Loss=0.976, Acc=0.742\n",
      "Step 3047: Loss=0.975, Acc=0.750\n",
      "Step 3056: Loss=0.975, Acc=0.746\n",
      "Step 3095: Loss=0.973, Acc=0.735\n",
      "Step 3097: Loss=0.961, Acc=0.742\n",
      "Step 3184: Loss=0.956, Acc=0.738\n",
      "Step 3190: Loss=0.939, Acc=0.751\n",
      "Step 3254: Loss=0.929, Acc=0.756\n",
      "Step 3301: Loss=0.929, Acc=0.744\n",
      "Step 3307: Loss=0.926, Acc=0.734\n",
      "Step 3357: Loss=0.919, Acc=0.731\n",
      "Step 3380: Loss=0.909, Acc=0.738\n",
      "Step 3384: Loss=0.896, Acc=0.750\n",
      "Step 3386: Loss=0.895, Acc=0.760\n",
      "Step 3406: Loss=0.888, Acc=0.753\n",
      "Step 3423: Loss=0.876, Acc=0.763\n",
      "Step 3442: Loss=0.874, Acc=0.763\n",
      "Step 3448: Loss=0.873, Acc=0.768\n",
      "Step 3474: Loss=0.862, Acc=0.770\n",
      "Step 3571: Loss=0.858, Acc=0.774\n",
      "Step 3632: Loss=0.857, Acc=0.773\n",
      "Step 3682: Loss=0.854, Acc=0.783\n",
      "Step 3777: Loss=0.853, Acc=0.778\n",
      "Step 3780: Loss=0.852, Acc=0.770\n",
      "Step 3801: Loss=0.847, Acc=0.779\n",
      "Step 3868: Loss=0.837, Acc=0.780\n",
      "Step 3900: Loss=0.834, Acc=0.786\n",
      "Step 3904: Loss=0.829, Acc=0.789\n",
      "Step 3943: Loss=0.824, Acc=0.788\n",
      "Step 4175: Loss=0.824, Acc=0.792\n",
      "Step 4272: Loss=0.821, Acc=0.787\n",
      "Step 4281: Loss=0.821, Acc=0.788\n",
      "Step 4286: Loss=0.807, Acc=0.798\n",
      "Step 4350: Loss=0.806, Acc=0.790\n",
      "Step 4378: Loss=0.799, Acc=0.795\n",
      "Step 4385: Loss=0.796, Acc=0.801\n",
      "Step 4387: Loss=0.794, Acc=0.801\n",
      "Step 4405: Loss=0.794, Acc=0.799\n",
      "Step 4409: Loss=0.791, Acc=0.798\n",
      "Step 4493: Loss=0.788, Acc=0.787\n",
      "Step 4565: Loss=0.771, Acc=0.810\n",
      "Step 4589: Loss=0.770, Acc=0.795\n",
      "Step 4684: Loss=0.768, Acc=0.798\n",
      "Step 4720: Loss=0.758, Acc=0.801\n",
      "Step 4733: Loss=0.758, Acc=0.796\n",
      "Step 4781: Loss=0.757, Acc=0.792\n",
      "Step 4810: Loss=0.757, Acc=0.791\n",
      "Step 4814: Loss=0.753, Acc=0.785\n",
      "Step 4867: Loss=0.751, Acc=0.801\n",
      "Step 4913: Loss=0.747, Acc=0.808\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "print(\"Starting training...\")\n",
    "best_loss = 999\n",
    "for i in range(5000):\n",
    "    # Random weight updates for all layers\n",
    "    model.w1 += 0.00885123 * np.random.randn(*model.w1.shape)\n",
    "    model.b1 += 0.00885123 * np.random.randn(*model.b1.shape)\n",
    "    model.w2 += 0.00885123 * np.random.randn(*model.w2.shape)\n",
    "    model.b2 += 0.00885123 * np.random.randn(*model.b2.shape)\n",
    "    model.w3 += 0.00885123 * np.random.randn(*model.w3.shape)\n",
    "    model.b3 += 0.00885123 * np.random.randn(*model.b3.shape)\n",
    "    \n",
    "    pred = model.forward(X)\n",
    "    loss = model.loss(pred, y)\n",
    "    \n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        model.save(\"Mohammad2237234Model.npz\")\n",
    "        acc = np.mean(np.argmax(pred, axis=1) == y)\n",
    "        print(f\"Step {i}: Loss={loss:.3f}, Acc={acc:.3f}\")\n",
    "    else:\n",
    "        model.load(\"Mohammad2237234Model.npz\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac193741",
   "metadata": {},
   "source": [
    "# **Loading Result Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c10f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(\"Mohammad2237234Model.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccb841",
   "metadata": {},
   "source": [
    "# **Test on shared test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3239585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shared test set...\n",
      "Shared Test Accuracy: 0.750\n"
     ]
    }
   ],
   "source": [
    "test_path = \"Ai2_Dataset/SharedTesting\"\n",
    "if os.path.exists(test_path):\n",
    "    print(\"Loading shared test set...\")\n",
    "    test_X, test_y = load_data(test_path)\n",
    "    test_pred = model.forward(test_X)\n",
    "    test_acc = np.mean(np.argmax(test_pred, axis=1) == test_y)\n",
    "    print(f\"Shared Test Accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfccc61d",
   "metadata": {},
   "source": [
    "# **Print every prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8cba0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Predicted=1, Actual=0\n",
      "Sample 1: Predicted=0, Actual=0\n",
      "Sample 2: Predicted=0, Actual=0\n",
      "Sample 3: Predicted=0, Actual=0\n",
      "Sample 4: Predicted=0, Actual=0\n",
      "Sample 5: Predicted=3, Actual=0\n",
      "Sample 6: Predicted=8, Actual=0\n",
      "Sample 7: Predicted=0, Actual=0\n",
      "Sample 8: Predicted=0, Actual=0\n",
      "Sample 9: Predicted=0, Actual=0\n",
      "Sample 10: Predicted=4, Actual=1\n",
      "Sample 11: Predicted=1, Actual=1\n",
      "Sample 12: Predicted=1, Actual=1\n",
      "Sample 13: Predicted=1, Actual=1\n",
      "Sample 14: Predicted=1, Actual=1\n",
      "Sample 15: Predicted=1, Actual=1\n",
      "Sample 16: Predicted=1, Actual=1\n",
      "Sample 17: Predicted=1, Actual=1\n",
      "Sample 18: Predicted=1, Actual=1\n",
      "Sample 19: Predicted=1, Actual=1\n",
      "Sample 20: Predicted=3, Actual=2\n",
      "Sample 21: Predicted=2, Actual=2\n",
      "Sample 22: Predicted=2, Actual=2\n",
      "Sample 23: Predicted=2, Actual=2\n",
      "Sample 24: Predicted=2, Actual=2\n",
      "Sample 25: Predicted=2, Actual=2\n",
      "Sample 26: Predicted=2, Actual=2\n",
      "Sample 27: Predicted=2, Actual=2\n",
      "Sample 28: Predicted=2, Actual=2\n",
      "Sample 29: Predicted=2, Actual=2\n",
      "Sample 30: Predicted=3, Actual=3\n",
      "Sample 31: Predicted=3, Actual=3\n",
      "Sample 32: Predicted=4, Actual=3\n",
      "Sample 33: Predicted=3, Actual=3\n",
      "Sample 34: Predicted=3, Actual=3\n",
      "Sample 35: Predicted=3, Actual=3\n",
      "Sample 36: Predicted=3, Actual=3\n",
      "Sample 37: Predicted=3, Actual=3\n",
      "Sample 38: Predicted=3, Actual=3\n",
      "Sample 39: Predicted=2, Actual=3\n",
      "Sample 40: Predicted=2, Actual=4\n",
      "Sample 41: Predicted=4, Actual=4\n",
      "Sample 42: Predicted=1, Actual=4\n",
      "Sample 43: Predicted=4, Actual=4\n",
      "Sample 44: Predicted=4, Actual=4\n",
      "Sample 45: Predicted=9, Actual=4\n",
      "Sample 46: Predicted=4, Actual=4\n",
      "Sample 47: Predicted=4, Actual=4\n",
      "Sample 48: Predicted=3, Actual=4\n",
      "Sample 49: Predicted=4, Actual=4\n",
      "Sample 50: Predicted=5, Actual=5\n",
      "Sample 51: Predicted=5, Actual=5\n",
      "Sample 52: Predicted=5, Actual=5\n",
      "Sample 53: Predicted=5, Actual=5\n",
      "Sample 54: Predicted=5, Actual=5\n",
      "Sample 55: Predicted=5, Actual=5\n",
      "Sample 56: Predicted=5, Actual=5\n",
      "Sample 57: Predicted=5, Actual=5\n",
      "Sample 58: Predicted=5, Actual=5\n",
      "Sample 59: Predicted=5, Actual=5\n",
      "Sample 60: Predicted=6, Actual=6\n",
      "Sample 61: Predicted=3, Actual=6\n",
      "Sample 62: Predicted=9, Actual=6\n",
      "Sample 63: Predicted=6, Actual=6\n",
      "Sample 64: Predicted=6, Actual=6\n",
      "Sample 65: Predicted=6, Actual=6\n",
      "Sample 66: Predicted=6, Actual=6\n",
      "Sample 67: Predicted=6, Actual=6\n",
      "Sample 68: Predicted=6, Actual=6\n",
      "Sample 69: Predicted=6, Actual=6\n",
      "Sample 70: Predicted=8, Actual=7\n",
      "Sample 71: Predicted=7, Actual=7\n",
      "Sample 72: Predicted=3, Actual=7\n",
      "Sample 73: Predicted=8, Actual=7\n",
      "Sample 74: Predicted=7, Actual=7\n",
      "Sample 75: Predicted=7, Actual=7\n",
      "Sample 76: Predicted=7, Actual=7\n",
      "Sample 77: Predicted=7, Actual=7\n",
      "Sample 78: Predicted=3, Actual=7\n",
      "Sample 79: Predicted=7, Actual=7\n",
      "Sample 80: Predicted=8, Actual=8\n",
      "Sample 81: Predicted=8, Actual=8\n",
      "Sample 82: Predicted=8, Actual=8\n",
      "Sample 83: Predicted=8, Actual=8\n",
      "Sample 84: Predicted=8, Actual=8\n",
      "Sample 85: Predicted=8, Actual=8\n",
      "Sample 86: Predicted=9, Actual=8\n",
      "Sample 87: Predicted=8, Actual=8\n",
      "Sample 88: Predicted=9, Actual=8\n",
      "Sample 89: Predicted=8, Actual=8\n",
      "Sample 90: Predicted=9, Actual=9\n",
      "Sample 91: Predicted=0, Actual=9\n",
      "Sample 92: Predicted=0, Actual=9\n",
      "Sample 93: Predicted=9, Actual=9\n",
      "Sample 94: Predicted=0, Actual=9\n",
      "Sample 95: Predicted=9, Actual=9\n",
      "Sample 96: Predicted=1, Actual=9\n",
      "Sample 97: Predicted=8, Actual=9\n",
      "Sample 98: Predicted=9, Actual=9\n",
      "Sample 99: Predicted=6, Actual=9\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for i, pred in enumerate(test_pred):\n",
    "    predicted_class = np.argmax(pred)\n",
    "    actual_class = test_y[i]\n",
    "    print(f\"Sample {i}: Predicted={predicted_class}, Actual={actual_class}\")\n",
    "else:\n",
    "    print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
